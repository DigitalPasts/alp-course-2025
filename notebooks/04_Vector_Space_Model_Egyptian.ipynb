{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Intro\n",
        "\n",
        "In this notebook, we explore a collection of ancient Akkadian and ancient Egyptian texts using the vector space model approach described by [Karsdorp et al. in the chapter \"Exploring Texts using the Vector Space Model\"](https://www.humanitiesdataanalysis.org/vector-space-model/notebook.html). By representing the texts as numeric vectors capturing word frequencies, we can quantify the lexical similarities and differences between corpora in each of these two ancient languages. The vector space model allows us to reason about texts spatially and apply geometric concepts like distance metrics to assess how \"close\" texts are to each other based on shared vocabulary.\n",
        "\n",
        "We preprocess the texts by tokenizing them into words, constructing a document-term matrix recording word frequencies per text, and analyzing the matrix using tools from the Python scientific computing stack, including NumPy, SciPy and Scikit-learn. Through techniques like tSNE (t-Distributed Stochastic Neighbor Embedding) and aggregation by text metadata like script type, language or genre, we explore patterns in the Akkadian and Egyptian corpora and showcase how the vector space model can yield quantitative insights into ancient textual data. The notebook serves as an example application of the concepts and methods covered in depth by Karsdorp et al. in their chapter.\n",
        "\n",
        "This notebook has been prepared by **Avital Romach** and is based on her research. It should be cited accordingly (see citation information at the bottom)."
      ],
      "metadata": {
        "id": "MWdwHvE2KbXH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing the corpus"
      ],
      "metadata": {
        "id": "3TBdIa1XSQos"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "Lw5XXlgeG-kQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nyHMlwmPGXC4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from scipy.spatial.distance import pdist, squareform\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "import plotly.express as px"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions"
      ],
      "metadata": {
        "id": "wO5SvgoORcfJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### To upload corpus and metadata from GitHub"
      ],
      "metadata": {
        "id": "d9CCW78rHKOl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Functions and import for the **Akkadian** corpus\n",
        "\n",
        "The Akkadian corpus consists of a part of the _[Royal Inscriptions of the Neo-Assyrian Period (RINAP)](https://colab.research.google.com/drive/14hTZCg-9XyiireusajDQqc9k2GAbc82e#scrollTo=qUcbzacX0kJy&line=3&uniqifier=1)_, licensed CC-BY-SA, and was taken from Open Richely Annotated Cuneiform Corpus (ORACC)."
      ],
      "metadata": {
        "id": "qUcbzacX0kJy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_corpus_from_github_api(url):\n",
        "  # URL on the Github where the csv files are stored\n",
        "  github_url = url\n",
        "  response = requests.get(github_url)\n",
        "\n",
        "  corpus = []\n",
        "  # Check if the request was successful\n",
        "  if response.status_code == 200:\n",
        "    files = response.json()\n",
        "    for file in files:\n",
        "      if file[\"download_url\"][-3:] == \"csv\":\n",
        "        corpus.append(pd.read_csv(file[\"download_url\"], encoding=\"utf-8\", index_col=\"Unnamed: 0\").fillna(\"\"))\n",
        "        # For Egyptian adapt like this:\n",
        "        #corpus.append(pd.read_csv(file[\"download_url\"], encoding=\"utf-8\").fillna(\"\"))\n",
        "  else:\n",
        "    print('Failed to retrieve files:', response.status_code)\n",
        "\n",
        "  return corpus\n",
        "\n",
        "def get_metadata_from_raw_github(url):\n",
        "  metadata = pd.read_csv(url, encoding=\"utf-8\", index_col=\"Unnamed: 0\").fillna(\"\")\n",
        "  return metadata"
      ],
      "metadata": {
        "id": "SBd1MSw6zT97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare Akkadian corpus (list of dataframes)\n",
        "\n",
        "corpus = create_corpus_from_github_api('https://api.github.com/repos/DigitalPasts/ALP-course/contents/course_notebooks/data/rinap01')\n",
        "corpus.extend(create_corpus_from_github_api('https://api.github.com/repos/DigitalPasts/ALP-course/contents/course_notebooks/data/rinap05'))\n"
      ],
      "metadata": {
        "id": "dojXSSkIxvgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare Akkadian metadata\n",
        "metadata = get_metadata_from_raw_github(\"https://raw.githubusercontent.com/DigitalPasts/ALP-course/master/course_notebooks/data/rinap1_5_metadata.csv\")\n"
      ],
      "metadata": {
        "id": "GixgIAtLyebb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Functions and import for the **Egyptian** corpus\n",
        "\n",
        "The Egyptian corpus is an extract of the database of the _[Thesaurus Linguae Aegyptiae (TLA)](https://thesaurus-linguae-aegyptiae.de)_, containing literary (and if you like: medical) texts. This export from the database is not published under a free license. Therefore, we access it from a private GitHub repository using an access token."
      ],
      "metadata": {
        "id": "Cb_DROcmxxBl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_corpus_from_private_github_api(url, token):\n",
        "# URL on the Github where the csv files are stored\n",
        "    headers = {\n",
        "        \"Authorization\": f\"token {token}\"\n",
        "    }\n",
        "    github_url = url\n",
        "    response = requests.get(github_url, headers=headers)\n",
        "\n",
        "    dtype_dict = {\"lemma_id\": \"str\"}\n",
        "\n",
        "    corpus = []\n",
        "    # Check if the request was successful\n",
        "    if response.status_code == 200:\n",
        "        files = response.json() # Github API provides information about the data in the repository, e.g. the download_url\n",
        "        for file in files:\n",
        "            if file[\"download_url\"][-3:] == \"csv\" or \".csv?token=\" in file[\"download_url\"]:\n",
        "                corpus.append(pd.read_csv(file[\"download_url\"], encoding=\"utf-8\", sep = ',', dtype=dtype_dict).fillna(\"\"))\n",
        "    else:\n",
        "        print('Failed to retrieve files:', response.status_code)\n",
        "\n",
        "    return corpus\n",
        "\n",
        "from io import StringIO\n",
        "\n",
        "def get_metadata_from_raw_private_github(url, token):\n",
        "    headers = {\n",
        "        \"Authorization\": f\"token {token}\"\n",
        "    }\n",
        "    github_url = url\n",
        "    response = requests.get(github_url, headers=headers)\n",
        "\n",
        "    # Check if the request was successful\n",
        "    if response.status_code == 200:\n",
        "        csv_data = StringIO(response.text)\n",
        "        metadata = pd.read_csv(csv_data, encoding=\"utf-8\", sep = ',', index_col=\"text_id\").fillna(\"\")\n",
        "        return metadata\n",
        "    else:\n",
        "        raise Exception(f\"Failed to retrieve metadata: {response.status_code}\")"
      ],
      "metadata": {
        "id": "A7PyzjwSrz8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# only if corpus is not yet loaded\n",
        "# Prepare Egyptian corpus (lists of dataframes)\n",
        "\n",
        "#if False:\n",
        "\n",
        "# NB: This token will expire at the end ot the year (2025)\n",
        "tla_access_token = \"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\"\n",
        "\n",
        "  ## TLA Literature\n",
        "corpus = create_corpus_from_private_github_api('https://api.github.com/repos/thesaurus-linguae-aegyptiae/test-rawdata/contents/alp-course-2024/TLA_literature/erzaehlungen', tla_access_token)\n",
        "\n",
        "corpus.extend(create_corpus_from_private_github_api('https://api.github.com/repos/thesaurus-linguae-aegyptiae/test-rawdata/contents/alp-course-2024/TLA_literature/reden', tla_access_token))\n",
        "\n",
        "corpus.extend(create_corpus_from_private_github_api('https://api.github.com/repos/thesaurus-linguae-aegyptiae/test-rawdata/contents/alp-course-2024/TLA_literature/lehren', tla_access_token))\n",
        "\n",
        "  ## TLA Medical\n",
        "  #corpus = create_corpus_from_private_github_api('https://api.github.com/repos/thesaurus-linguae-aegyptiae/test-rawdata/contents/alp-course-2024/TLA_medical/TLA_pEbers', tla_access_token)\n",
        "\n",
        "  #corpus.extend(create_corpus_from_private_github_api('https://api.github.com/repos/thesaurus-linguae-aegyptiae/test-rawdata/contents/alp-course-2024/TLA_medical/TLA_pEdwinSmith', tla_access_token))\n"
      ],
      "metadata": {
        "id": "xnqvAC0Lr_nr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Egyptian metadata\n",
        "metadata = get_metadata_from_raw_private_github(\"https://raw.githubusercontent.com/thesaurus-linguae-aegyptiae/test-rawdata/master/alp-course-2024/TLA_literature/TLA_metadata.csv\", tla_access_token)\n"
      ],
      "metadata": {
        "id": "-4bDluLUy2hv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Check if data is loaded\n",
        "corpus[0].head()"
      ],
      "metadata": {
        "id": "OYUZPigulF_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare text_ids (list of unique ids), and metadata\n",
        "\n",
        "text_ids = []\n",
        "for text in corpus:\n",
        "  text_ids.append(text[\"text\"].iloc[0])\n",
        "\n",
        "\n",
        "for id in text_ids:\n",
        "  if id not in metadata.index:\n",
        "    print(f\"Text {id} missing from metadata\")\n",
        "\n",
        "metadata = metadata[metadata.index.isin(text_ids)]\n",
        "\n",
        "metadata"
      ],
      "metadata": {
        "id": "FgnIN_DKGRMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### To convert dataframe to string"
      ],
      "metadata": {
        "id": "LD-SVUMViYnS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is necessary because `TfidfVectorizer` that we will use to do the tf-idf calculations requires a list of strings as input. Each string is an entire text (document).\n",
        "\n",
        "**Function to split the text dataframes according to a column**. Used to separate text to lines:\n",
        "* param df: dataframe containing one word in each row.\n",
        "* param column: the column by which to split the dfs, perferably `text` or `line`.\n",
        "* return: a list of dataframes split according to the value given to the column parameter.\n",
        "\n"
      ],
      "metadata": {
        "id": "ICeHGzO7h1tn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_df_by_column_value(df, column):\n",
        "\n",
        "    dfs = []\n",
        "    column_values = df[column].unique()\n",
        "    for value in column_values:\n",
        "        split_df = df[df[column]==value]\n",
        "        dfs.append(split_df)\n",
        "    return dfs"
      ],
      "metadata": {
        "id": "F6QGuzdwiCXe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_df_by_column_value(corpus[0].head(), \"line\")"
      ],
      "metadata": {
        "id": "mxoSOTE_jKTA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Function to convert the values from the text dataframe to a string of text with or without line breaks and word segmentation**.\n",
        "* param df: the text dataframe\n",
        "* param column: the chosen column from the dataframe to construct the text from (preferably unicode_word, cf, or lemma)\n",
        "* param break_perc: a parameter which dictates whether to include broken words depending on the percentage of how broken they are.\n",
        "                       Compares this value to the `break_perc` column in the dataframe.\n",
        "                       Parameter is set to 1 (i.e. all words, whether broken or not, are included); can be any float between 0 and 1.\n",
        "* param mask: boolean whether to mask named entities or not; set to True.\n",
        "* return: a string which includes all the words in the texts according to the column chosen. Extra spaces that were between broken words or empty lines are removed."
      ],
      "metadata": {
        "id": "245HmRQRihqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def df2str(df, column, break_perc=1, mask=True, segmentation=True):\n",
        "\n",
        "    # check if column exists in dataframe. If not, return empty text.\n",
        "    if column not in df.columns:\n",
        "        return (\"\", 0, 0)\n",
        "    else:\n",
        "        # remove rows that include duplicate values for compound words\n",
        "        if column not in [\"norm\", \"cf\", \"sense\", \"pos\"]:\n",
        "            df = df.drop_duplicates(\"ref\").copy()\n",
        "        # if column entry is empty string, replace with UNK (can happen with normalization or lemmatization)\n",
        "        mask_empty = df[column]==\"\"\n",
        "        df[column] = df[column].where(~mask_empty, other=\"UNK\")\n",
        "        # mask proper nouns\n",
        "        if mask and \"pos\" in df.columns:\n",
        "            mask_bool = df[\"pos\"].isin([\"PN\", \"RN\", \"DN\", \"GN\", \"MN\", \"SN\", \"n\"])\n",
        "            df[column] = df[column].where(~mask_bool, other=df[\"pos\"])\n",
        "\n",
        "        # change number masking from `n` to `NUM`\n",
        "        # !comment out for Egyptian\n",
        "        #if mask:\n",
        "        #    mask_num = df[column]==\"n\"\n",
        "        #    df[column] = df[column].where(~mask_num, other=\"NUM\")\n",
        "\n",
        "        # remove rows without break_perc (happens with non-Akkadian words)\n",
        "        if \"\" in df[\"break_perc\"].unique():\n",
        "            df = df[df[\"break_perc\"]!=\"\"].copy()\n",
        "        # filter according to break_perc\n",
        "        mask_break = df[\"break_perc\"] <= break_perc\n",
        "        df[column] = df[column].where(mask_break, other=\"X\")\n",
        "        # calculate text length with and without UNK and x tokens\n",
        "        text_length_full = df.shape[0]\n",
        "        mask_partial = df[column].isin([\"UNK\", \"X\", \"x\"])\n",
        "        text_length_partial = text_length_full - sum(mask_partial)\n",
        "        # create text lines\n",
        "        text = \"\"\n",
        "        df_lines = split_df_by_column_value(df, \"line\")\n",
        "        for line in df_lines:\n",
        "            word_list = list(filter(None, line[column].to_list()))\n",
        "            if word_list != []:\n",
        "                text += \" \".join(map(str, word_list)).replace(\"x\", \"X\").strip() + \" \" #+ \"\\n\"\n",
        "\n",
        "        if segmentation == False:\n",
        "            # remove all white spaces (word segmentation and line breaks)\n",
        "            text = re.sub(r\"[\\s\\u00A0]+\", \"\", text)\n",
        "\n",
        "        return (text, text_length_full, text_length_partial)"
      ],
      "metadata": {
        "id": "ZhXsDitEiu9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2str(corpus[0], \"lemma_id\")"
      ],
      "metadata": {
        "id": "QkUpMMp5jIKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### To convert to specific word levels and create dictionaries"
      ],
      "metadata": {
        "id": "-UGZ10MnjEG5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Function to convert the dataframes into strings of lemmatized texts**.\n",
        "* param corpus: a list of dataframes\n",
        "* param break_perc: a parameter which dictates whether to include broken words depending on the percentage of how broken they are.\n",
        "                       Compares this value to the `break_perc` column in the dataframe.\n",
        "                       Parameter is set to 1 (i.e. all words, whether broken or not, are included); can be any float between 0 and 1.\n",
        "* param mask: boolean whether to mask named entities or not; set to True.\n",
        "* return: a dictionary where the keys are the text IDs and the values are the lemmatized texts"
      ],
      "metadata": {
        "id": "IWIlEgvZCGGO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_lemmatized_texts(corpus, break_perc=1, mask=True):\n",
        "\n",
        "    texts_dict = {}\n",
        "    for df in corpus:\n",
        "        # get the text number from the dataframe \"text\" column\n",
        "        key = df[\"text\"].iloc[0]\n",
        "        text, text_length_full, text_length_partial = df2str(df, \"lemma_id\", break_perc, mask)\n",
        "        texts_dict[key] = (text, text_length_full, text_length_partial)\n",
        "    return texts_dict"
      ],
      "metadata": {
        "id": "KjEVlACkBtBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_lemmatized_texts((split_df_by_column_value(corpus[0], \"text\")))"
      ],
      "metadata": {
        "id": "6Bl6MhgICuFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Function to convert the dataframes into strings of normalized texts**.\n",
        "* param corpus: a list of dataframes\n",
        "* param break_perc: a parameter which dictates whether to include broken words depending on the percentage of how broken they are.\n",
        "                       Compares this value to the `break_perc` column in the dataframe.\n",
        "                       Parameter is set to 1 (i.e. all words, whether broken or not, are included); can be any float between 0 and 1.\n",
        "* param mask: boolean whether to mask named entities or not; set to True.\n",
        "* return: a dictionary where the keys are the text IDs and the values are the normalized texts"
      ],
      "metadata": {
        "id": "cU-mpEjFDlLx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_normalized_texts(corpus, break_perc=1, mask=True):\n",
        "\n",
        "    texts_dict = {}\n",
        "    for df in corpus:\n",
        "        # get the text number from the dataframe \"text\" column\n",
        "        key = df[\"text\"].iloc[0]\n",
        "        text, text_length_full, text_length_partial = df2str(df, \"norm\", break_perc, mask)\n",
        "        texts_dict[key] = (text, text_length_full, text_length_partial)\n",
        "    return texts_dict"
      ],
      "metadata": {
        "id": "2iJAkjhUDuDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_normalized_texts((split_df_by_column_value(corpus[0], \"text\")))"
      ],
      "metadata": {
        "id": "7cJpNzRvDyFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Function to convert the dataframes into strings of segmented unicode texts**.\n",
        "* param corpus: a list of dataframes\n",
        "* param break_perc: a parameter which dictates whether to include broken words depending on the percentage of how broken they are.\n",
        "                       Compares this value to the `break_perc` column in the dataframe.\n",
        "                       Parameter is set to 1 (i.e. all words, whether broken or not, are included); can be any float between 0 and 1.\n",
        "* param mask: boolean whether to mask named entities or not; set to True.\n",
        "* return: a dictionary where the keys are the text IDs and the values are the segmented unicode texts"
      ],
      "metadata": {
        "id": "51W1Q2EmEg5N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_segmented_unicode_texts(corpus, break_perc=1, mask=True):\n",
        "\n",
        "    texts_dict = {}\n",
        "    for df in corpus:\n",
        "        # get the text number from the dataframe \"text\" column\n",
        "        key = df[\"text\"].iloc[0]\n",
        "        text, text_length_full, text_length_partial = df2str(df, \"unicode_word\", break_perc, mask)\n",
        "        texts_dict[key] = (text, text_length_full, text_length_partial)\n",
        "    return texts_dict"
      ],
      "metadata": {
        "id": "68fqoZrdEggT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_segmented_unicode_texts((split_df_by_column_value(corpus[0], \"text\")))"
      ],
      "metadata": {
        "id": "XKUSmEOaE8sE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### To create the vector space model"
      ],
      "metadata": {
        "id": "PMCMiNQE5kbj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Vectorizing texts with TfidfVectorizer\n",
        "\n",
        "🔧 What Does TfidfVectorizer Do?\n",
        "\n",
        "TfidfVectorizer is a class that:\n",
        "\n",
        "   * Reads text data\n",
        "   * Cleans and tokenizes it\n",
        "   * Builds a vocabulary\n",
        "   * Calculates TF-IDF values\n",
        "   * Returns a matrix (lokks similar to a Pandas dataframe but isn't a dataframe) where each row is a document and each column is a term"
      ],
      "metadata": {
        "id": "s9ioJeo0KqOn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Converts a list of texts into a term-document matrix based on TF-IDF scores**.\n",
        "\n",
        "Full documentation of the variables of TfidfVectorizer from sklearn, see: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer\n",
        "* param corpus: a dataframe in which the texts are in a `\"text\"` column and the dataframe's index is the text ids.\n",
        "* param analyzer: whether the feature should be made of word or character n-grams.\n",
        "                     use `\"word\"` for word features, `\"char_wb\"` for character n-grams within word boundaries,\n",
        "                     or `\"char\"` for character n-grams without word boundaries.\n",
        "* param ngram_range: the lower and upper boundary of the range of n-values for different n-grams to be extracted.\n",
        "* param max_df: threshold to ignore terms that have a document frequency above a certain value.\n",
        "                   If the threshold is a float, it represent a proportion of the documents.\n",
        "                   If the threshold is an integer, it represents absolute counts of number of documents in which the terms appears.\n",
        "* param min_df: threshold to ignore terms that have a document frequency below a certain value.\n",
        "                   If the threshold is a float, it represent a proportion of the documents.\n",
        "                   If the threshold is an integer, it represents absolute counts of number of documents in which the terms appears.\n",
        "* param max_features: if not `None`, build a vocabulary that only considers the top max_features ordered by term frequency across the corpus.\n",
        "* param stop_words: if `None`, no stop words are used. Otherwise, can be a list with words to be removed from resulting tokens.\n",
        "* return: `counts` the raw counts of the vectorizer,\n",
        "             `counts_df` a dataframe of the counts where the index is the text ids and the columns are the tokens,\n",
        "             `stop_words` an updated list of stop words"
      ],
      "metadata": {
        "id": "jVN9XLRlJ9D8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://www.humanitiesdataanalysis.org/_images/bow.png)\n",
        "\n"
      ],
      "metadata": {
        "id": "nJquCdqxOwrn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Figure 1**. Example of a document-term matrix extracted from a corpus, see Fig. 3 in Karsdorp, F., Kestemont, M., & Riddell, A. (2021). Humanities Data Analysis: Case Studies\n",
        "with Python. Princeton University Press."
      ],
      "metadata": {
        "id": "m1ZSucGrQpE1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize(corpus, analyzer=\"word\", ngram_range=(1,1), max_df=1.0, min_df=1, max_features=None, stop_words=[\"UNK\", \"X\"]):\n",
        "\n",
        "    vectorizer = TfidfVectorizer(input=\"content\", lowercase=False, analyzer=analyzer,\n",
        "                                 # RegEx for Akkadian\n",
        "                                 #token_pattern=r\"(?u)\\b\\w+\\b\", ngram_range=ngram_range,\n",
        "                                 # RegEx for Egyptian\n",
        "                                 token_pattern=r\"(?u)\\b[\\w\\.]+\\b\", ngram_range=ngram_range,\n",
        "                                 max_df=max_df, min_df=min_df, max_features=max_features, stop_words=stop_words)\n",
        "\n",
        "    counts = vectorizer.fit_transform(corpus[\"text\"].tolist()).toarray()\n",
        "    #stop_words = vectorizer.stop_words_ # use when stop_words are not defined in the parameters\n",
        "\n",
        "    # saving the vocab used for vectorization, and switching the dictionary so that the feature index is the key\n",
        "    vocab = vectorizer.vocabulary_\n",
        "    switched_vocab = {value: key for key, value in vocab.items()}\n",
        "    # adding the vocab words to the counts dataframe for easier viewing.\n",
        "    column_names = []\n",
        "    x = 0\n",
        "    while x < len(switched_vocab):\n",
        "        column_names.append(switched_vocab[x])\n",
        "        x += 1\n",
        "\n",
        "    counts_df = pd.DataFrame(counts, index=corpus.index, columns=column_names)\n",
        "\n",
        "    return (counts, counts_df, stop_words)"
      ],
      "metadata": {
        "id": "ekWEVWPrK1B-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Calculating distances between vectorized documents"
      ],
      "metadata": {
        "id": "M38qD4s0K9tv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Converts a term-document matrix to a text similarity matrix**.\n",
        "* param counts: the raw counts from the `vectorize` function.\n",
        "* param metric: the metric by which to calculate the distances between the texts in the corpus. For one place to look into the different types of matrics see \"Computing distances between documents\" in [Karsdrop, Kestemont, & Riddell 2021](https://www.humanitiesdataanalysis.org/vector-space-model/notebook.html#computing-distances-between-documents)\n",
        "                   Valid metrics are:\n",
        "                   ‘braycurtis’, ‘canberra’, ‘chebyshev’, ‘cityblock’, ‘correlation’, ‘cosine’,\n",
        "                   ‘dice’, ‘euclidean’, ‘hamming’, ‘jaccard’, ‘jensenshannon’, ‘kulczynski1’,\n",
        "                   ‘mahalanobis’, ‘matching’, ‘minkowski’, ‘rogerstanimoto’, ‘russellrao’,\n",
        "                   ‘seuclidean’, ‘sokalmichener’, ‘sokalsneath’, ‘sqeuclidean’, ‘yule’.\n",
        "* param text_ids: list of unique text_ids.\n",
        "* return: a dataframe matrix of distance between texts."
      ],
      "metadata": {
        "id": "9LuEi5HRLEWY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def distance_calculator(counts, metric, text_ids):\n",
        "\n",
        "    return pd.DataFrame(squareform(pdist(counts, metric=metric)), index=text_ids, columns=text_ids)"
      ],
      "metadata": {
        "id": "JtgbjirqLDjG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### reducing dimensions with pca or tsne"
      ],
      "metadata": {
        "id": "3oFCyz3rLasY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reduces multidimensional data into two dimensions using PCA**.\n",
        "* param df: dataframe holding the dimensions to reduce. All columns should include numerical values only.\n",
        "               The dataframe's index should hold the unique text ids.\n",
        "* param metadata: the rest of the metadata in the corpus, to help visualize the resulting clusters in meaningful ways.\n",
        "                     The metadata's index should hold the unique text ids.\n",
        "* return: a dataframe with the coordinates of the two remaining dimensions on all other columns from the metadata."
      ],
      "metadata": {
        "id": "J-CdhqfwLjhF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reduce_dimensions_pca(df, metadata):\n",
        "\n",
        "    pca = PCA(n_components=2)\n",
        "    reduced_data = pca.fit_transform(df)\n",
        "    reduced_df = pd.DataFrame(data=reduced_data, index=df.index, columns=[\"component 1\", \"component 2\"])\n",
        "    reduced_df_metadata = metadata.join(reduced_df)\n",
        "    return reduced_df_metadata"
      ],
      "metadata": {
        "id": "Sn1l5Rp_Lbac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reduces multidimensional data into two dimensions using TSNE**.\n",
        "\n",
        "See full documentation of sklearn's TSNE on: https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html\n",
        "* param df: dataframe holding the dimensions to reduce. All columns should include numerical values only.\n",
        "               The dataframe's index should hold the unique text ids.\n",
        "* param perplexity: perplexity is a measure the weighs the importance of nearby versus distant points when creating a lower-dimension mapping.\n",
        "                       t-SNE first converts the distances between points into conditional probabilities that represent similarities,\n",
        "                       using Gaussian probability distributions.\n",
        "                       The perplexity parameter influences the variance used to compute these probabilities.\n",
        "                       A higher perplexity leads to a broader Gaussian that considers a larger number of neighbors when assessing similarity.\n",
        "                       Lower perplexity puts more focus on the local structure and considers fewer neighbors.\n",
        "                       A good perplexity depends greatly on dataset size and density.\n",
        "                       The documentation recommends a value between 5 and 50.\n",
        "                       We recommend to start with the square root of the length of the corpus.\n",
        "* param n_iter: maximum number of iterations for optimization.\n",
        "* param metric: the metric to be used when calculating distances between vectors.\n",
        "                   Valid metrics are:\n",
        "                   ‘braycurtis’, ‘canberra’, ‘chebyshev’, ‘cityblock’, ‘correlation’, ‘cosine’,\n",
        "                   ‘dice’, ‘euclidean’, ‘hamming’, ‘jaccard’, ‘jensenshannon’, ‘kulczynski1’,\n",
        "                   ‘mahalanobis’, ‘matching’, ‘minkowski’, ‘rogerstanimoto’, ‘russellrao’,\n",
        "                   ‘seuclidean’, ‘sokalmichener’, ‘sokalsneath’, ‘sqeuclidean’, ‘yule’.\n",
        "* param metadata: the rest of the metadata in the corpus, to help visualize the resulting clusters in meaningful ways.\n",
        "                     The metadata's index should hold the unique text ids.\n",
        "* return: a dataframe with the coordinates of the two remaining dimensions on all other columns from the metadata."
      ],
      "metadata": {
        "id": "fvp4qrkULy6a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reduce_dimensions_tsne(df, perplexity, n_iter, metric, metadata):\n",
        "\n",
        "    tsne = TSNE(n_components=2, perplexity=perplexity, n_iter=n_iter, metric=metric, init=\"pca\")\n",
        "    reduced_data = tsne.fit_transform(df)\n",
        "    reduced_df = pd.DataFrame(data=reduced_data, index=df.index, columns=[\"component 1\", \"component 2\"])\n",
        "    reduced_df_metadata = metadata.join(reduced_df)\n",
        "    return reduced_df_metadata"
      ],
      "metadata": {
        "id": "BWEwUvt65p1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Process texts from dataframes and combine results with metadata dataframe"
      ],
      "metadata": {
        "id": "Jng_wsFahhjs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to combine processed texts with metadata\n",
        "\n",
        "def get_corpus_metadata(texts_dict, metadata):\n",
        "  texts_df = pd.DataFrame(texts_dict, index=[\"text\", \"full_length\", \"partial_length\"]).transpose()\n",
        "  df = metadata.join(texts_df)\n",
        "  return df"
      ],
      "metadata": {
        "id": "mWzJBzU7HJqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## vectorize lemma forms\n",
        "corpus_dict = get_lemmatized_texts(corpus, break_perc=0)\n",
        "## vectorize normalized forms\n",
        "#corpus_dict = get_normalized_texts(corpus, break_perc=0)\n",
        "## vectorize Unicode cuneiform\n",
        "#corpus_dict = get_segmented_unicode_texts(corpus, break_perc=0)\n",
        "\n",
        "corpus_metadata = get_corpus_metadata(corpus_dict, metadata)\n",
        "\n",
        "## For Akkadian\n",
        "## remove texts which have less than n words excluding UNK and X\n",
        "#n = 10\n",
        "#print(f\"Number of texts before filtering: {corpus_metadata.shape[0]}\")\n",
        "#corpus_metadata = corpus_metadata[corpus_metadata[\"partial_length\"]>=n]\n",
        "#print(f\"Number of texts after filtering: {corpus_metadata.shape[0]}\")\n",
        "\n",
        "\n",
        "# For Egyptian use this instead, reset the index\n",
        "n = 150\n",
        "print(f\"Number of texts before filtering: {corpus_metadata.shape[0]}\")\n",
        "corpus_metadata = corpus_metadata[corpus_metadata[\"partial_length\"]>=n].set_index(\"text_name\")\n",
        "print(f\"Number of texts after filtering: {corpus_metadata.shape[0]}\")"
      ],
      "metadata": {
        "id": "Q1i9wtNCxJWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_metadata"
      ],
      "metadata": {
        "id": "adGbPDc4FSwZ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploring the Akkadian RINAP or Egyptian TLA Corpus using the Vector Space Model"
      ],
      "metadata": {
        "id": "cRwY7N32zfQR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# vectorize corpus\n",
        "counts, counts_df, stop_words = vectorize(corpus_metadata, max_features=50)"
      ],
      "metadata": {
        "id": "mm3YZQGs4x1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "counts_df.head(3)"
      ],
      "metadata": {
        "id": "PZtqUY6uNj7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate distance between vectorized texts\n",
        "matrix = distance_calculator(counts, \"cosine\", corpus_metadata.index)\n",
        "matrix"
      ],
      "metadata": {
        "id": "MFoQOq7w5QnY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize matrix\n",
        "fig = px.imshow(matrix)\n",
        "\n",
        "# adjust size of the matrix\n",
        "fig.update_layout(\n",
        "    autosize=False,\n",
        "    width=1500,\n",
        "    height=1500,\n",
        ")\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "aP65l1sGDdIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reduce matrix dimensions\n",
        "reduced_tsne = reduce_dimensions_tsne(matrix, perplexity=matrix.shape[0]**0.5, n_iter=5000, metric=\"euclidean\", metadata=corpus_metadata)"
      ],
      "metadata": {
        "id": "zodSXjV_Dsco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize reduced dimensions\n",
        "\n",
        "# adjust size column for visualization\n",
        "size_min = 3\n",
        "size_max = 70\n",
        "size = (reduced_tsne[\"partial_length\"] / reduced_tsne[\"partial_length\"].max() * (size_max - size_min) + size_min).tolist()\n",
        "\n",
        "# create figure\n",
        "# for Akkadian use symbol = \"script\", color=\"project\",\n",
        "# for Egyptian\n",
        "fig = px.scatter(reduced_tsne, x=\"component 1\", y=\"component 2\", size=size, symbol = \"corpus_manual\", color=\"language_manual\", hover_data=[\"partial_length\", \"full_length\", reduced_tsne.index])\n",
        "fig.update_traces(marker=dict(line=dict(width=1, color='black')))\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "R2sfDFXxRFZX",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Find Shared Tokens"
      ],
      "metadata": {
        "id": "vzE4jn2aywPY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**creates a mini df that includes only the chosen text and the shared tokens in those texts**\n",
        "  (i.e., all tokens that are none zero in all texts).\n",
        "* param df: the counts_df where the index is the text ids and the columns are the tokens.\n",
        "* param text_ids: a list containing text ids.\n",
        "* return: a dataframe where the index are the shared tokens and the columns are the texts.\n",
        "           the values are the tf-idf scores."
      ],
      "metadata": {
        "id": "stQkSTdPIFeF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_shared_tokens(df, text_ids):\n",
        "\n",
        "  mini_df = df[df.index.isin(text_ids)].copy()\n",
        "  mini_df = mini_df.loc[:, (mini_df != 0).all(axis=0)].copy()\n",
        "  return mini_df.transpose()"
      ],
      "metadata": {
        "id": "O1luauCfQOJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Akkadian\n",
        "#shared_tokens = find_shared_tokens(counts_df, [\"Q003450\", \"Q003711\", \"Q003790\"])\n",
        "# Egyptian\n",
        "shared_tokens = find_shared_tokens(counts_df, [\"pBerlin P 3023 + pAmherst I (Bauer, B1) || Der beredte Bauer (Version B1)\", \"pBerlin P 3025 + pAmherst II (Bauer, B2) || Der beredte Bauer (Version B2)\", \"pPetersburg 1116 A || Verso: Die Lehre für Merikare\", \"pBM EA 10509 (Ptahhotep, Version L2+L2G) || Die Lehre des Ptahhotep (Version L2+L2G)\"])#, \"3RU7Z4VQ45CYFIQ4PUGQ3HDJFU\"])\n",
        "\n",
        "shared_tokens"
      ],
      "metadata": {
        "id": "TrUxwzx80mWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "px.scatter(shared_tokens)"
      ],
      "metadata": {
        "id": "-uJwKsBC2usV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*This notebook was created by [Avital Romach](https://github.com/ARomach), with additional code and text by [Eliese-Sophia Lincke](https://www.geschkult.fu-berlin.de/e/aegyptologie/personen/Professorinnen-und-Professoren/Lincke/index.html), [Shai Gordin](https://digitalpasts.github.io/) and [Daniel A. Werning](https://www.bbaw.de/die-akademie/mitarbeiterinnen-mitarbeiter/werning-daniel) in Spring 2024 for the course [Ancient Language Processing](https://digitalpasts.github.io/ALP-course/). Code can be reused under a [CC-BY 4.0](https://creativecommons.org/licenses/by/4.0/)*"
      ],
      "metadata": {
        "id": "rHqa0j0mMm7x"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q-IOsnZuQyfB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}