{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rkhO60dmHc8m"
   },
   "source": [
    "# Brushing up on your Python Skills\n",
    "\n",
    "The basics of this class are taught in Python. And the neglected basics of ALP is preprocessing our texts.\n",
    "\n",
    "Preprocessing for ALP is much broader than what computer and data scientists usually mean. Philological conventions in printed and digital publications hold much more information that needs to be correctly parsed before any computational manipulation (analysis).\n",
    "\n",
    "In this notebook, we are going to provide four examples of messy texts: two in Egyptian and two in Akkadian. We are going to work through the process of how we should parse the texts, what information we are losing when parsing them, and brushing up on basic Python syntax and functions while we're at it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RZMX2K-gMg5j"
   },
   "source": [
    "## Akkadian Example 1:\n",
    "\n",
    "https://cdli.mpiwg-berlin.mpg.de/artifacts/225104\n",
    "\n",
    "&P225104 = TIM 10, 134\n",
    "#atf: use lexical\n",
    "#Nippur 2N-T496; proverb; Alster proverbs\n",
    "@tablet\n",
    "@obverse\n",
    "@column 1\n",
    "1. dub-sar hu-ru\n",
    "2. a-ga-asz-gi4-gi4-me!(|ME+ASZ|)-e-ne\n",
    "3. dub-sar hu-ru\n",
    "4. a-ga-asz-gi4-gi4-me!(|ME+ASZ|)-e#-ne\n",
    "@reverse\n",
    "@column 1\n",
    "1. igi-bi 3(disz) 3(asz) 6(disz)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QUwK2e7_PlFr"
   },
   "source": [
    "### Task 1:\n",
    "\n",
    "How do we turn this raw text into a list of words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T09:02:48.320107Z",
     "start_time": "2024-05-01T09:02:48.308147Z"
    },
    "id": "Ytf3w5-dN61H",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "akk1 = \"\"\"&P225104 = TIM 10, 134\n",
    "#atf: use lexical\n",
    "#Nippur 2N-T496; proverb; Alster proverbs\n",
    "@tablet\n",
    "@obverse\n",
    "@column 1\n",
    "1. dub-sar hu-ru\n",
    "2. a-ga-asz-gi4-gi4-me!(|ME+ASZ|)-e-ne\n",
    "3. dub-sar hu-ru\n",
    "4. a-ga-asz-gi4-gi4-me!(|ME+ASZ|)-e#-ne\n",
    "@reverse\n",
    "@column 1\n",
    "1. igi-bi 3(disz) 3(asz) 6(disz)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T09:02:48.327591Z",
     "start_time": "2024-05-01T09:02:48.322910Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "34ethpugN99a",
    "outputId": "a7253cbe-7867-439d-8acd-d6077257f121",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "akk1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T09:02:48.333142Z",
     "start_time": "2024-05-01T09:02:48.328971Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C-8h5EXYPgSE",
    "outputId": "9c2b6d38-7d20-4183-bd83-e6e012eb96c2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# split string to lines of texts\n",
    "lines = akk1.split(\"\\n\")\n",
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T09:02:48.338604Z",
     "start_time": "2024-05-01T09:02:48.334503Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1WQbRMV_RT7L",
    "outputId": "824cdfe1-577c-4832-db58-1c2fd9ce60d2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# remove blanks\n",
    "\n",
    "lines_full = []\n",
    "for line in lines:\n",
    "  if line != \"\":\n",
    "    lines_full.append(line)\n",
    "\n",
    "lines_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T09:02:48.348320Z",
     "start_time": "2024-05-01T09:02:48.341651Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W2kEguvRQmND",
    "outputId": "a188c992-f45c-40d8-deed-2c129a4c51e2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# keep only lines that begin with a number\n",
    "# use regular expressions\n",
    "\n",
    "import re\n",
    "\n",
    "text_lines = []\n",
    "for line in lines_full:\n",
    "  if re.match(\"^\\d\", line) != None:\n",
    "    text_lines.append(line)\n",
    "\n",
    "text_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T09:02:48.354536Z",
     "start_time": "2024-05-01T09:02:48.350763Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W0zRcypOTKla",
    "outputId": "d349e544-0a06-4abd-9bb1-2d84dfcfaf33",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# separate lines into words\n",
    "\n",
    "words_appended = []\n",
    "words_extended = []\n",
    "for line in text_lines:\n",
    "  temp_words = line.split()\n",
    "  words_appended.append(temp_words[1:]) # creates list of lists\n",
    "  words_extended.extend(temp_words[1:]) # creates list\n",
    "\n",
    "print(words_appended)\n",
    "print(\"-------------------------------\")\n",
    "print(words_extended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T09:02:48.394344Z",
     "start_time": "2024-05-01T09:02:48.381948Z"
    },
    "id": "FXkWg0eVdqJN",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# rewrite the code above as a function\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7r568N_MVNFS"
   },
   "source": [
    "What information did we lose when preprocessing the texts in this way?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xa5fciAXVT4b"
   },
   "source": [
    "### Task 2:\n",
    "\n",
    "Create a dictionary from the raw texts, of the following format:\n",
    "\n",
    "```\n",
    "{\"pnum\": ...\n",
    " \"textID\": ...\n",
    " \"surface\": [{\n",
    "  \"surfaceType\": ...\n",
    "  \"columns\": [{\n",
    "    \"columnNum\": ...\n",
    "    \"text\": [{\n",
    "      \"lineNum\": ...\n",
    "      \"words\": [..., ..., ...]\n",
    "    }]\n",
    "  }]\n",
    " }]}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T09:02:48.400040Z",
     "start_time": "2024-05-01T09:02:48.396596Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NPWuL8lWYH-U",
    "outputId": "d7cffabf-7eb3-4325-d176-5750872f0756",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# separate text into lines\n",
    "\n",
    "lines = akk1.split(\"\\n\")\n",
    "\n",
    "lines_full = []\n",
    "for line in lines:\n",
    "  if line != \"\":\n",
    "    lines_full.append(line)\n",
    "\n",
    "lines_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T09:02:48.404146Z",
     "start_time": "2024-05-01T09:02:48.401115Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 216
    },
    "id": "5iLR_THGWkxZ",
    "outputId": "b7ccf196-4dae-4451-fc69-a2694827ed1c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# store the pnum and textID in variables\n",
    "\n",
    "text_ids = lines_full[0]\n",
    "pnum, textID = text_ids.split(\"=\")\n",
    "\n",
    "pnum = pnum.strip()[1:]\n",
    "textID = textID.strip()\n",
    "print(pnum)\n",
    "print(textID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T09:02:48.409129Z",
     "start_time": "2024-05-01T09:02:48.405294Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JYBJnaxNX8pi",
    "outputId": "7eb8e64d-7d67-4875-94a8-5f83391615e1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create a dictionary for each surface (simple no regex method)\n",
    "# what do you do when you have different types of inscribed object? (e.g. cylinder, prism, bowl, slab, etc.)\n",
    "\n",
    "valid_surface_values = [\"@obverse\", \"@reverse\"]\n",
    "\n",
    "surface_idx = []\n",
    "\n",
    "for index, line in enumerate(lines_full):\n",
    "  if line in valid_surface_values: # what is dangerous in this line? if the line of text is not exactly(!) part of surface, no lines will be found\n",
    "    surface_idx.append(index)\n",
    "print(surface_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T09:02:48.414356Z",
     "start_time": "2024-05-01T09:02:48.410610Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iaukt_6QdNuO",
    "outputId": "a406b317-41c3-46d9-f8a1-e40caae7273d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create a dictionary for each surface (complicated with regex method)\n",
    "# what do you do when you have different type of inscribed object? (e.g. cylinder, prism, bowl, slab, etc.)\n",
    "\n",
    "valid_surface_values = [\"@obverse\", \"@reverse\"]\n",
    "\n",
    "pattern = r\"^(?:\" + \"|\".join([re.escape(value) for value in valid_surface_values]) + \")\" # This is called a list comprehension\n",
    "\n",
    "surface_idx = []\n",
    "\n",
    "for index, line in enumerate(lines_full): # returns the index for the line and the content of the line\n",
    "  if re.match(pattern, line) != None:\n",
    "    surface_idx.append(index)\n",
    "print(surface_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T09:32:21.282331Z",
     "start_time": "2024-05-01T09:32:21.265494Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# same code like in cell above but without list comprehension\n",
    "# create a dictionary for each surface (complicated with regex method)\n",
    "# what do you do when you have different type of inscribed object? (e.g. cylinder, prism, bowl, slab, etc.)\n",
    "\n",
    "valid_surface_values = [\"@obverse\", \"@reverse\"]\n",
    "\n",
    "#pattern = r\"^(?:\" + \"|\".join([re.escape(value) for value in valid_surface_values]) + \")\" # This is called a list comprehension\n",
    "\n",
    "escaped_values = []\n",
    "for value in valid_surface_values:\n",
    "    escaped_values.append(re.escape(value))\n",
    "print(escaped_values)\n",
    "\n",
    "pattern = r\"^(?:\" + \"|\".join(escaped_values) + \")\"\n",
    "    \n",
    "surface_idx = []\n",
    "\n",
    "for index, line in enumerate(lines_full): # returns the index for the line and the content of the line\n",
    "  if re.match(pattern, line) != None:\n",
    "    surface_idx.append(index)\n",
    "print(surface_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T09:02:48.430830Z",
     "start_time": "2024-05-01T09:02:48.422183Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IGKS6eNpdgfD",
    "outputId": "00ab26a1-db45-42ab-c477-93d43e8ae551",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# use surface indices to create surface dictionaries\n",
    "# surfaceType; columnNum; lineNum; words\n",
    "# surfaceType extracted using id values of lines\n",
    "# columnNum needs first to check whether a column actually exists, then extracted using regex(?)/tokenize on space for any number after the word column\n",
    "# lineNum is regex for any line that begins with a number plus any tags attached: how would be best to define line numbers, as integers or as string variables?\n",
    "# words extracted from each text line after lineNum using regex and tokenized on spaces\n",
    "\n",
    "for index, id in enumerate(surface_idx):\n",
    "    surfaceType = lines_full[id].replace('@', '')\n",
    "    print(index, id)\n",
    "    if index < len(surface_idx) - 1:\n",
    "        end_of_surface = surface_idx[index+1]\n",
    "    else:\n",
    "        end_of_surface = len(lines_full)\n",
    "\n",
    "    # Extract the text content for the current surface designation\n",
    "    surface_content = lines_full[id+1:end_of_surface]\n",
    "\n",
    "    # Print the surface type and its content\n",
    "    print(f\"Surface Type: {surfaceType}\")\n",
    "    # print(\"Content:\")\n",
    "    # print('\\n'.join(surface_content))\n",
    "    print('---')\n",
    "\n",
    "    # Extract column number, line numbers, and words for each surface content\n",
    "    for line in surface_content:\n",
    "        columnNum = None\n",
    "        lineNum = None\n",
    "        words = []\n",
    "\n",
    "        # Check if the line contains a column number\n",
    "        if '@column' in line:\n",
    "            parts = line.split()\n",
    "            if len(parts) >= 2:\n",
    "                try:\n",
    "                    columnNum = int(parts[1])\n",
    "                except ValueError:\n",
    "                    pass\n",
    "            print(f\"Column Number: {columnNum}\")\n",
    "            print('---')\n",
    "            continue  # Skip processing the line with @column\n",
    "\n",
    "        # Check if the line contains a line number\n",
    "        if '.' in line:\n",
    "            parts = line.split('.')\n",
    "            if len(parts) >= 2:\n",
    "                lineNum = parts[0].strip()\n",
    "\n",
    "        # Tokenize the words in the line\n",
    "        if lineNum:\n",
    "            words = parts[1].strip().split()\n",
    "        else:\n",
    "            words = line.strip().split()\n",
    "\n",
    "        # Print the extracted information for each line\n",
    "        print(f\"Line Number: {lineNum}\")\n",
    "        print(f\"Words: {words}\")\n",
    "        print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T09:02:48.443092Z",
     "start_time": "2024-05-01T09:02:48.436306Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nl8Qq9wbPIXy",
    "outputId": "2c2661f3-aa64-450b-c3b1-93fcd9c69f87",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Combine the surfaces and metadata into one dictionary\n",
    "\n",
    "output = {\n",
    "    \"pnum\": pnum,\n",
    "    \"textID\": textID,\n",
    "    \"surface\": []\n",
    "}\n",
    "\n",
    "for index, id in enumerate(surface_idx):\n",
    "    surfaceType = lines_full[id].replace('@', '')\n",
    "    surface = {\n",
    "        \"surfaceType\": surfaceType,\n",
    "        \"columns\": []\n",
    "    }\n",
    "\n",
    "    if index < len(surface_idx) - 1:\n",
    "        end_of_surface = surface_idx[index+1]\n",
    "    else:\n",
    "        end_of_surface = len(lines_full)\n",
    "\n",
    "    # Extract the text content for the current surface designation\n",
    "    surface_content = lines_full[id+1:end_of_surface]\n",
    "\n",
    "    # Extract column number, line numbers, and words for each surface content\n",
    "    columnNum = None\n",
    "    column = {\n",
    "        \"columnNum\": None,\n",
    "        \"text\": []\n",
    "    }\n",
    "    for line in surface_content:\n",
    "        lineNum = None\n",
    "        words = []\n",
    "\n",
    "        # Check if the line contains a column number\n",
    "        if '@column' in line:\n",
    "            parts = line.split()\n",
    "            if len(parts) >= 2:\n",
    "                try:\n",
    "                    columnNum = int(parts[1])\n",
    "                    column[\"columnNum\"] = columnNum\n",
    "                except ValueError:\n",
    "                    pass\n",
    "            continue  # Skip processing the line with @column\n",
    "\n",
    "        # Check if the line contains a line number\n",
    "        if '.' in line:\n",
    "            parts = line.split('.')\n",
    "            if len(parts) >= 2:\n",
    "                lineNum = parts[0].strip()\n",
    "\n",
    "        # Tokenize the words in the line\n",
    "        if lineNum:\n",
    "            words = parts[1].strip().split()\n",
    "        else:\n",
    "            words = line.strip().split()\n",
    "\n",
    "        # Add the line information to the column\n",
    "        line_info = {\n",
    "            \"lineNum\": lineNum,\n",
    "            \"words\": words\n",
    "        }\n",
    "        column[\"text\"].append(line_info)\n",
    "\n",
    "    # Add the column to the surface\n",
    "    surface[\"columns\"].append(column)\n",
    "\n",
    "    # Add the surface to the output\n",
    "    output[\"surface\"].append(surface)\n",
    "\n",
    "# Print the output in the specified dictionary format\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T09:02:48.449539Z",
     "start_time": "2024-05-01T09:02:48.444656Z"
    },
    "id": "plIJDngOZr7z",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save the output dictionary as a JSON file\n",
    "\n",
    "import json\n",
    "with open(f\"{pnum}.json\", \"w\") as json_file:\n",
    "    json.dump(output, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T09:02:48.453242Z",
     "start_time": "2024-05-01T09:02:48.451033Z"
    },
    "id": "ljeV7ovmdyX9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# rewrite the code above into a function\n",
    "\n",
    "print(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sAHFgXUIe4Ta"
   },
   "source": [
    "## Egyptian Example 1:\n",
    "\n",
    "A sentence from the sarcophagus of the Napatan king Aspelta (c. 600-580 BCE), found in his pyramid in Nuri, Sudan (Nu. 8), https://collections.mfa.org/objects/145117\n",
    "\n",
    "Get the context of the sentence from the Thesaurus Linguae Aegyptiae: https://thesaurus-linguae-aegyptiae.de/text/27KHHMEP4VHSDH737F2OFLKNSE/sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T09:02:48.481434Z",
     "start_time": "2024-05-01T09:02:48.455213Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This Dictionary was created from the original json file\n",
    "\n",
    "eg1 = {'publication_statement': {'credit_citation': 'Doris Topmann, Sentence ID 2CBOF5UQ7JGETCXG2CQKPCWDZM <https://github.com/thesaurus-linguae-aegyptiae/tla-raw-data/blob/v17/sentences/2CBOF5UQ7JGETCXG2CQKPCWDZM.json>, in: Thesaurus Linguae Aegyptiae: Raw Data <https://github.com/thesaurus-linguae-aegyptiae/tla-raw-data>, Corpus issue 17 (31 October 2022), ed. by Tonio Sebastian Richter & Daniel A. Werning on behalf of the Berlin-Brandenburgische Akademie der Wissenschaften and Hans-Werner Fischer-Elfert & Peter Dils on behalf of the S√§chsische Akademie der Wissenschaften zu Leipzig (first published: 22 September 2023)', 'collection_editors': 'Tonio Sebastian Richter & Daniel A. Werning on behalf of the Berlin-Brandenburgische Akademie der Wissenschaften and Hans-Werner Fischer-Elfert & Peter Dils on behalf of the S√§chsische Akademie der Wissenschaften zu Leipzig', 'data_engineers': {'input_software_BTS': ['Christoph Plutte', 'Jakob H√∂per'], 'database_curation': ['Simon D. Schweitzer'], 'data_transformation': ['Jakob H√∂per', 'R. Dominik Bl√∂se', 'Daniel A. Werning']}, 'date_published_in_TLA': '2022-10-31', 'rawdata_first_published': '2023-09-22', 'corresponding_TLA_URL': 'https://thesaurus-linguae-aegyptiae.de/sentence/2CBOF5UQ7JGETCXG2CQKPCWDZM', 'license': 'Creative Commons Attribution-ShareAlike 4.0 International (CC BY-SA 4.0) <https://creativecommons.org/licenses/by-sa/4.0/>'}, 'context': {'line': 'III', 'paragraph': None, 'pos': 7, 'textId': '27KHHMEP4VHSDH737F2OFLKNSE', 'textType': 'Text', 'variants': 1}, 'eclass': 'BTSSentence', 'glyphs': {'mdc_compact': None, 'unicode': None}, 'id': '2CBOF5UQ7JGETCXG2CQKPCWDZM', 'relations': {'contains': [{'eclass': 'BTSAnnotation', 'id': 'DYJEAXFKBJAXJPVLJGWREJZJ5M', 'ranges': [{'end': 'OKLGJLCEQFHU7HDRYUTYR352YA', 'start': '22TFIMS2CBBCFFCDSCAIT3HR3Y'}], 'type': '√§gyptologische Textsegmentierung'}], 'partOf': [{'eclass': 'BTSText', 'id': '27KHHMEP4VHSDH737F2OFLKNSE', 'name': 'Isis (HT 15, HT 14, HT 17)', 'type': 'Text'}]}, 'tokens': [{'annoTypes': ['√§gyptologische Textsegmentierung'], 'flexion': {'btsGloss': '(unspecified)', 'lingGloss': 'PTCL', 'numeric': 3}, 'glyphs': {'mdc_artificially_aligned': True, 'mdc_compact': 'D35:N35', 'mdc_original': 'D35-N35', 'mdc_original_safe': None, 'mdc_tla': 'D35-N35', 'order': [1, 2], 'unicode': 'ìÇúìàñ'}, 'id': '22TFIMS2CBBCFFCDSCAIT3HR3Y', 'label': 'nn', 'lemma': {'POS': {'type': 'particle'}, 'id': '851961'}, 'transcription': {'mdc': 'nn', 'unicode': 'nn'}, 'translations': {'de': ['[Negationspartikel]']}, 'type': 'word'}, {'annoTypes': ['√§gyptologische Textsegmentierung'], 'flexion': {'btsGloss': 'SC.act.ngem.nom.subj_Neg.nn', 'lingGloss': 'V\\\\tam.act', 'numeric': 210020}, 'glyphs': {'mdc_artificially_aligned': False, 'mdc_compact': 'W11-V28-A7', 'mdc_original': 'W11-V28-A7', 'mdc_original_safe': None, 'mdc_tla': 'W11-V28-A7', 'order': [2, 3, 4], 'unicode': 'ìéºìéõìÄâ'}, 'id': 'IOLUGQXLCRGNLMTAPJ65LI7MHU', 'label': 'g·∏•', 'lemma': {'POS': {'subtype': 'verb_3-lit', 'type': 'verb'}, 'id': '166480'}, 'transcription': {'mdc': 'gH', 'unicode': 'g·∏•'}, 'translations': {'de': ['matt sein']}, 'type': 'word'}, {'annoTypes': ['√§gyptologische Textsegmentierung'], 'flexion': {'btsGloss': 'Noun.pl.stpr.3sgm', 'lingGloss': 'N.f:pl:stpr', 'numeric': 70154}, 'glyphs': {'mdc_artificially_aligned': True, 'mdc_compact': 'D36:X1*F51B-Z2', 'mdc_original': 'D36-X1-F51B-Z2', 'mdc_original_safe': None, 'mdc_tla': 'D36-X1-F51B-Z2', 'order': [5, 6, 7, 8], 'unicode': 'ìÇùìèèìÑπÔ∏Ä\\U00013440ìè•'}, 'id': 'GUVBJUGCSVF5VN55PN6RYS4YLI', 'label': 'Íú•,t.pl', 'lemma': {'POS': {'subtype': 'substantive_fem', 'type': 'substantive'}, 'id': '34550'}, 'transcription': {'mdc': 'a.t.PL', 'unicode': 'Íú•.t.PL'}, 'translations': {'de': ['Glied; K√∂rperteil']}, 'type': 'word'}, {'annoTypes': ['√§gyptologische Textsegmentierung'], 'flexion': {'btsGloss': '(unspecified)', 'lingGloss': '-3sg.m', 'numeric': 3}, 'glyphs': {'mdc_artificially_aligned': False, 'mdc_compact': 'I9', 'mdc_original': 'I9', 'mdc_original_safe': None, 'mdc_tla': 'I9', 'order': [9], 'unicode': 'ìÜë'}, 'id': 'GIHCJ27JXVAM7GDUYWGEPKBRB4', 'label': '=f', 'lemma': {'POS': {'subtype': 'personal_pronoun', 'type': 'pronoun'}, 'id': '10050'}, 'transcription': {'mdc': '=f', 'unicode': '=f'}, 'translations': {'de': ['[Suffix Pron. sg.3.m.]']}, 'type': 'word'}, {'annoTypes': ['√§gyptologische Textsegmentierung'], 'flexion': {'btsGloss': '(unspecified)', 'lingGloss': 'dem.f.pl', 'numeric': 3}, 'glyphs': {'mdc_artificially_aligned': True, 'mdc_compact': 'M17-Q3:N35', 'mdc_original': 'M17-Q3-N35', 'mdc_original_safe': None, 'mdc_tla': 'M17-Q3-N35', 'order': [10, 11, 12], 'unicode': 'ìáãìä™ìàñ'}, 'id': 'Z6HTGGPBPRDT3OZTZNXRF2GRDA', 'label': 'jp‚å©t‚å™n', 'lemma': {'POS': {'subtype': 'demonstrative_pronoun', 'type': 'pronoun'}, 'id': '850009'}, 'transcription': {'mdc': 'jp‚å©t‚å™n', 'unicode': 'jp‚å©t‚å™n'}, 'translations': {'de': ['diese [Dem.Pron. pl.f.]']}, 'type': 'word'}, {'annoTypes': ['√§gyptologische Textsegmentierung'], 'flexion': {'btsGloss': '(unspecified)', 'lingGloss': 'TITL', 'numeric': 3}, 'glyphs': {'mdc_artificially_aligned': False, 'mdc_compact': 'D4-Q1-A40', 'mdc_original': 'D4-Q1-A40', 'mdc_original_safe': None, 'mdc_tla': 'D4-Q1-A40', 'order': [13, 14, 15], 'unicode': 'ìÅπìä®ìÄ≠'}, 'id': 'UCFJWBLRKJG4NJWTWT22WDR2MU', 'label': 'Wsr,w', 'lemma': {'POS': {'subtype': 'title', 'type': 'epitheton_title'}, 'id': '49461'}, 'transcription': {'mdc': 'wsr.w', 'unicode': 'Wsr.w'}, 'translations': {'de': ['Osiris (Totentitel des Verstorbenen)']}, 'type': 'word'}, {'annoTypes': ['√§gyptologische Textsegmentierung'], 'flexion': {'btsGloss': '(unspecified)', 'lingGloss': 'N', 'numeric': 3}, 'glyphs': {'mdc_artificially_aligned': True, 'mdc_compact': 'M23-X1:N35', 'mdc_original': 'M23-X1-N35', 'mdc_original_safe': None, 'mdc_tla': 'M23-X1-N35', 'order': [16, 17, 18], 'unicode': 'ìáììèèìàñ'}, 'id': 'LI5FJI4ZUJEMPIKS5RQ5HHNBUE', 'label': 'nzw', 'lemma': {'POS': {'type': 'substantive'}, 'id': '88040'}, 'transcription': {'mdc': 'nzw', 'unicode': 'nzw'}, 'translations': {'de': ['K√∂nig']}, 'type': 'word'}, {'annoTypes': ['√§gyptologische Textsegmentierung'], 'flexion': {'btsGloss': '(unspecified)', 'lingGloss': 'ROYLN', 'numeric': 3}, 'glyphs': {'mdc_artificially_aligned': True, 'mdc_compact': 'V30:N17-N17', 'mdc_original': 'V30-N17-N17', 'mdc_original_safe': None, 'mdc_tla': 'V30-N17-N17', 'order': [19, 20, 21], 'unicode': 'ìéüìáøìáø'}, 'id': 'ICADWHGbHkfdokpooG4eCy3Zfe8', 'label': 'nb-TÍú£,du', 'lemma': {'POS': {'subtype': 'epith_king', 'type': 'epitheton_title'}, 'id': '400038'}, 'transcription': {'mdc': 'nb-tA.DU', 'unicode': 'nb-TÍú£.DU'}, 'translations': {'de': ['Herr der Beiden L√§nder (K√∂nige)']}, 'type': 'word'}, {'annoTypes': ['√§gyptologische Textsegmentierung'], 'flexion': {'btsGloss': '(unspecified)', 'lingGloss': 'TITL', 'numeric': 3}, 'glyphs': {'mdc_artificially_aligned': True, 'mdc_compact': 'V30:D4-Aa1*X1:Y1', 'mdc_original': 'V30-D4-Aa1-X1-Y1', 'mdc_original_safe': None, 'mdc_tla': 'V30-D4-Aa1-X1-Y1', 'order': [22, 23, 24, 25, 26], 'unicode': 'ìéüìÅπìêçìèèìèõ'}, 'id': 'ICADWHT2O1dc30SXuRZUlquIDpM', 'label': 'nb-jr(,t)-(j)·∏´,t', 'lemma': {'POS': {'subtype': 'title', 'type': 'epitheton_title'}, 'id': '400354'}, 'transcription': {'mdc': 'nb-jr(.t)-(j)x.t', 'unicode': 'nb-jr(.t)-(j)·∏´.t'}, 'translations': {'de': ['Herr des Rituals']}, 'type': 'word'}, {'annoTypes': ['√§gyptologische Textsegmentierung'], 'flexion': {'btsGloss': '(unspecified)', 'lingGloss': 'ROYLN', 'numeric': 3}, 'glyphs': {'mdc_artificially_aligned': True, 'mdc_compact': '<-M17-O34:Q3-E23-N17->', 'mdc_original': '<-M17-O34-Q3-E23-N17->', 'mdc_original_safe': None, 'mdc_tla': '<-M17-O34-Q3-E23-N17->', 'order': [18, 19, 20, 21, 22, 23], 'unicode': 'ìçπ\\U0001343cìáãìäÉìä™ìÉ≠ìáø\\U0001343dìç∫'}, 'id': 'J3MLYALWVNAMDDG33VZ3RIEEUA', 'label': 'Jsplt', 'lemma': {'POS': {'subtype': 'kings_name', 'type': 'entity_name'}, 'id': '850103'}, 'transcription': {'mdc': 'jsplt', 'unicode': 'Jsplt'}, 'translations': {'de': ['Aspelta']}, 'type': 'word'}, {'annoTypes': ['√§gyptologische Textsegmentierung'], 'flexion': {'btsGloss': '(unspecified)', 'lingGloss': 'N.m:sg', 'numeric': 3}, 'glyphs': {'mdc_artificially_aligned': True, 'mdc_compact': 'U5:D36-P8h', 'mdc_original': 'U5-D36-P8h', 'mdc_original_safe': None, 'mdc_tla': 'U5-D36-P8h', 'order': [25, 26, 27], 'unicode': 'ìå∑ìÇùìä§Ô∏Ç'}, 'id': 'OKLGJLCEQFHU7HDRYUTYR352YA', 'label': 'mÍú£Íú•-·∏´rw', 'lemma': {'POS': {'subtype': 'substantive_masc', 'type': 'substantive'}, 'id': '66750'}, 'transcription': {'mdc': 'mAa-xrw', 'unicode': 'mÍú£Íú•-·∏´rw'}, 'translations': {'de': ['Gerechtfertigter (der selige Tote)']}, 'type': 'word'}], 'transcription': {'mdc': 'nn gH a.t.PL=f jp‚å©t‚å™n wsr.w nzw nb-tA.DU nb-jr(.t)-(j)x.t jsplt mAa-xrw', 'unicode': 'nn g·∏• Íú•.t.PL=f jp‚å©t‚å™n Wsr.w nzw nb-TÍú£.DU nb-jr(.t)-(j)·∏´.t Jsplt mÍú£Íú•-·∏´rw'}, 'translations': {'de': ['Diese seine Glieder werden nicht matt sein, (die des) Osiris K√∂nigs, des Herrn der Beiden L√§nder, des Herrn des Rituals, Aspelta, des Gerechtfertigten.']}, 'type': None, 'wordCount': 11, 'editors': {'author': 'Doris Topmann', 'contributors': None, 'created': '2020-12-23 12:24:26', 'type': None, 'updated': '2022-08-29 10:22:01'}}\n",
    "print(eg1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T09:02:48.487302Z",
     "start_time": "2024-05-01T09:02:48.483332Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# parse the dictionary (json)\n",
    "\n",
    "unicodeHiero = []\n",
    "transcription = []\n",
    "translLemma = []\n",
    "posLemma = []\n",
    "tokenID = []\n",
    "\n",
    "for text_word in eg1[\"tokens\"] :\n",
    "    print(text_word[\"glyphs\"][\"unicode\"], text_word[\"transcription\"][\"unicode\"], text_word[\"translations\"][\"de\"][0], text_word[\"lemma\"][\"POS\"][\"type\"], text_word[\"id\"] )\n",
    "    tokenID.append(text_word[\"id\"])\n",
    "    unicodeHiero.append(text_word[\"glyphs\"][\"unicode\"])\n",
    "    translLemma.append(text_word[\"translations\"][\"de\"][0])\n",
    "    posLemma.append(text_word[\"lemma\"][\"POS\"][\"type\"])\n",
    "    \n",
    "    if text_word[\"transcription\"][\"unicode\"][0] == \"=\" : # replace equal sign as it will cause trouble in spreadsheet software like MS Excel\n",
    "        transcription.append(text_word[\"transcription\"][\"unicode\"].replace(\"=\", '‚∏ó')) # U+2E17\n",
    "    else :\n",
    "        transcription.append(text_word[\"transcription\"][\"unicode\"])\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T09:02:48.490623Z",
     "start_time": "2024-05-01T09:02:48.488683Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get the ID of this sentence\n",
    "\n",
    "sentenceID = eg1[\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T09:02:48.969581Z",
     "start_time": "2024-05-01T09:02:48.492477Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create a dataframe and fill it\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df_eg = pd.DataFrame({\n",
    "    'unicode_hieroglyphs': unicodeHiero,\n",
    "    'unicode_transcription': transcription,\n",
    "    'lemma_translation': translLemma,\n",
    "    'part-of-speech': posLemma,\n",
    "    'tokenID' : tokenID\n",
    "})\n",
    "\n",
    "df_eg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T09:02:48.976861Z",
     "start_time": "2024-05-01T09:02:48.970774Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# save as *.csv\n",
    "\n",
    "fileName = \"aspelta_TLA_Sentence_\" + sentenceID + \".csv\"\n",
    "df_eg.to_csv(fileName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lgqdyMX1Kq_8"
   },
   "source": [
    "## Akkadian Example 2:\n",
    "\n",
    "consider the following Akkadian text:\n",
    "\n",
    "http://www.achemenet.com//fr/item/?/sources-textuelles/textes-par-publication/Strassmaier_Cyrus/1665118\n",
    "\n",
    " 6 udu-nita<sub>2</sub> <i>ina</i> ≈°u<sup>II</sup> <sup>Id</sup>en-gi a-<i>≈°√∫ ≈°√°</i> <sup>Id</sup>[\n",
    "\n",
    " <i>a-na</i> 8 g√≠n 4-<i>t√∫ </i>k√π-babbar<i> i-na</i> k√π-babbar\n",
    "\n",
    " <i>≈°√°</i> <i>i-di</i> √© [ o o o ]<i> a-na</i> √©-babbar-ra\n",
    "\n",
    " <i>it-ta-din</i> 5 udu-nita<sub>2</sub> <i>≈°√°</i> <sup>I</sup><i>ka-·π£ir</i>\n",
    "\n",
    " a-<i>≈°√∫ ≈°√°</i> <sup>Id</sup>en-mu<i> a-na</i> 7 g√≠n 4-<i>t√∫</i>\n",
    "\n",
    " k√π-babbar <i>≈°√°</i> <i>muh-hi</i> <i>dul-lu</i> <sup>I</sup>mu-mu\n",
    "\n",
    " <i>√∫-≈°√°-hi-su a-na</i> <i>l√¨b-bi</i> s√¨-<i>na</i>\n",
    "\n",
    " 1 udu-nita<sub>2</sub><i> a-na</i> 1 g√≠n 4-<i>t√∫ </i>k√π-babbar\n",
    "\n",
    " <i>ina</i> ≈°u<sup>II</sup> <sup>Id</sup>utu-ba-<i>≈°√°</i><sup>!</sup> [\n",
    "\n",
    " 1 udu-nita<sub>2</sub> <i>≈°√°</i> <sup>I</sup>DU-[\n",
    "\n",
    " <i>a-na</i> 1<sup>?</sup> g√≠n [\n",
    "\n",
    " pap [13 udu-nita<sub>2</sub>-me≈°\n",
    "\n",
    " iti du<sub>6</sub> u<sub>4</sub> [o-kam] mu sag nam-lugal-la\n",
    "\n",
    " <sup>I</sup><i>ku-ra-√°≈°</i> lugal tin-tir<sup>ki</sup> <i>u</i> kur-kur\n",
    "\n",
    "How would you preprocess this text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T09:26:38.626941Z",
     "start_time": "2024-05-01T09:26:38.617531Z"
    }
   },
   "outputs": [],
   "source": [
    "## raw text\n",
    "\n",
    "akk2 = \"\"\"6 udu-nita<sub>2</sub> <i>ina</i> ≈°u<sup>II</sup> <sup>Id</sup>en-gi a-<i>≈°√∫ ≈°√°</i> <sup>Id</sup>[\n",
    "<i>a-na</i> 8 g√≠n 4-<i>t√∫ </i>k√π-babbar<i> i-na</i> k√π-babbar \n",
    "<i>≈°√°</i> <i>i-di</i> √© [ o o o ]<i> a-na</i> √©-babbar-ra \n",
    "<i>it-ta-din</i> 5 udu-nita<sub>2</sub> <i>≈°√°</i> <sup>I</sup><i>ka-·π£ir</i> \n",
    "a-<i>≈°√∫ ≈°√°</i> <sup>Id</sup>en-mu<i> a-na</i> 7 g√≠n 4-<i>t√∫</i> \n",
    "k√π-babbar <i>≈°√°</i> <i>muh-hi</i> <i>dul-lu</i> <sup>I</sup>mu-mu \n",
    "<i>√∫-≈°√°-hi-su a-na</i> <i>l√¨b-bi</i> s√¨-<i>na</i> \n",
    "1 udu-nita<sub>2</sub><i> a-na</i> 1 g√≠n 4-<i>t√∫ </i>k√π-babbar \n",
    "<i>ina</i> ≈°u<sup>II</sup> <sup>Id</sup>utu-ba-<i>≈°√°</i><sup>!</sup> [\n",
    "1 udu-nita<sub>2</sub> <i>≈°√°</i> <sup>I</sup>DU-[\n",
    "<i>a-na</i> 1<sup>?</sup> g√≠n [\n",
    "pap [13 udu-nita<sub>2</sub>-me≈°\n",
    "iti du<sub>6</sub> u<sub>4</sub> [o-kam] mu sag nam-lugal-la \n",
    "<sup>I</sup><i>ku-ra-√°≈°</i> lugal tin-tir<sup>ki</sup> <i>u</i> kur-kur\"\"\"\n",
    "\n",
    "akk2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T09:26:39.251124Z",
     "start_time": "2024-05-01T09:26:39.233972Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Clean the raw text in akk2\n",
    "\n",
    "akk2 = akk2.replace(\"<sub>\", \"\")\n",
    "akk2 = akk2.replace(\"</sub>\", \"\")\n",
    "\n",
    "## Harmonize word and sign boundaries\n",
    "\n",
    "# Shift blank before/after <i>/</i>\n",
    "akk2 = akk2.replace(\" </i>\", \"</i> \")\n",
    "akk2 = akk2.replace(\"<i> \", \" <i>\")\n",
    "#print(akk2)\n",
    "\n",
    "import re\n",
    "# Add hyphen before <sup> tags if there is no space before the tag\n",
    "akk2 = re.sub(r'([^ ])(<sup>)', r'\\1-\\2', akk2)              \n",
    "# Add hyphen after </sup> and </i> tags if there is no space after the tag\n",
    "akk2 = re.sub(r'(</sup>|</i>)([^ ])', r'\\1-\\2', akk2)\n",
    "\n",
    "# from a-<i>≈°√∫ ≈°√°</i> to a-<i>≈°√∫</i> <i>≈°√°</i>\n",
    "pattern = r\"(<i>[^<]*)([ -])\"\n",
    "while True:\n",
    "    new_text = re.sub(pattern, r\"\\1</i>\\2<i>\", akk2)\n",
    "    if new_text == akk2:  # End loop if there are no more differences between the existing one and the one created by re substitution\n",
    "        break\n",
    "    akk2 = new_text\n",
    "\n",
    "# Replace double hyphens by simple ones\n",
    "akk2 = akk2.replace(\"--\", \"-\")\n",
    "\n",
    "print(akk2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T09:38:21.419364Z",
     "start_time": "2024-05-01T09:38:21.399006Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Create a dictionary with annotations\n",
    "\n",
    "akk2_lineList = akk2.split(\"\\n\")\n",
    "\n",
    "lines_list = []\n",
    "line_count = 1\n",
    "for line in akk2_lineList:\n",
    "    temp_words = line.split()\n",
    "    line_dict = {}\n",
    "    \n",
    "    line_dict['line_id'] = line_count\n",
    "    line_dict['words'] = temp_words\n",
    "\n",
    "    lines_list.append(line_dict)\n",
    "    line_count += 1\n",
    "\n",
    "#print(lines_list)\n",
    "\n",
    "for line in lines_list :\n",
    "    subword_list = []\n",
    "    word_count = 1\n",
    "    \n",
    "    for word in line['words'] :\n",
    "        subword_dict = {}\n",
    "        \n",
    "        sign_list = []\n",
    "        \n",
    "        if '-' in word: # if more than one sign, separated by hyphen\n",
    "            temp_signs = word.split('-')\n",
    "            sign_list.extend(temp_signs)\n",
    "        else : # if only individual sign \n",
    "            sign_list.append(word)\n",
    "        \n",
    "        signs_per_word = []\n",
    "        for sign in sign_list :\n",
    "            signs_per_word.append(sign)\n",
    "        \n",
    "        subword_dict['word_id'] = word_count\n",
    "        word_count += 1  \n",
    "        subword_dict['signs'] = signs_per_word\n",
    "        \n",
    "        list_sign_func_dict = []\n",
    "        for sign in subword_dict['signs'] :\n",
    "            sign_func_dict = {}\n",
    "            #print(sign)\n",
    "            if sign.startswith('<i>') and sign.endswith('</i>') :\n",
    "                sign_func_dict['sign'] = sign[3:-4]\n",
    "                sign_func_dict['sign_function'] = 'phonogram'\n",
    "               # sign = \n",
    "            elif sign.startswith('<sup>') and sign.endswith('</sup>') :\n",
    "                sign_func_dict['sign'] = sign[5:-6]\n",
    "                sign_func_dict['sign_function'] = 'classifier'\n",
    "            else:\n",
    "                sign_func_dict['sign'] = sign\n",
    "                sign_func_dict['sign_function'] = 'logogram'\n",
    "                \n",
    "            list_sign_func_dict.append(sign_func_dict)\n",
    "        #print(list_sign_func_dict)\n",
    "        \n",
    "            \n",
    "        subword_dict['signs'] = list_sign_func_dict\n",
    "        subword_list.append(subword_dict)\n",
    "\n",
    "    line['words'] = subword_list\n",
    "    \n",
    "\n",
    "#print(lines_list)    \n",
    "for line in lines_list:\n",
    "    print(line)\n",
    "    #for words in line:\n",
    "    #    print(line['words'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cDXxfebde9kR"
   },
   "source": [
    "## Egyptian Example 2:\n",
    "\n",
    "How to deal with non-Unicode hieroglyphs (<g> tag + Gardiner number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T09:02:49.013299Z",
     "start_time": "2024-05-01T09:02:49.013291Z"
    },
    "id": "e6CfHB0KNVaf",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eg2_csv = \"\"\",text,line,word,ref,frag,norm,unicode_word,unicode,lemma_id,cf,pos,sense\n",
    "92,3Z5EM77HJFCOPKZDDZFEMI6KVY,5,7,3Z5EM77HJFCOPKZDDZFEMI6KVY.5.7,gÍú£uÃØ.w,gÍú£uÃØ.w,<g>V96</g>ìÖ±,\"['<', 'g', '>', 'V', '9', '6', '<', '/', 'g', '>', 'ìÖ±']\",166210,gÍú£uÃØ,VERB,eng sein; entbehren; (jmdn.) Not leiden lassen\n",
    "151,4WVXFJZFLNAYHP3Y5O5SLWD7DA,2,2,4WVXFJZFLNAYHP3Y5O5SLWD7DA.2.2,nÍú•w,nÍú•w,ìàñìÇùìÖ±<g>I14C</g>ìè§,\"['ìàñ', 'ìÇù', 'ìÖ±', '<', 'g', '>', 'I', '1', '4', 'C', '<', '/', 'g', '>', 'ìè§']\",80510,NÍú•w,PROPN,Sich windender (Personifikation der Schlange)\n",
    "153,4WVXFJZFLNAYHP3Y5O5SLWD7DA,2,5,4WVXFJZFLNAYHP3Y5O5SLWD7DA.2.5,nÍú•w,nÍú•w,ìàñìÇùìÖ±<g>I14C</g>ìè§,\"['ìàñ', 'ìÇù', 'ìÖ±', '<', 'g', '>', 'I', '1', '4', 'C', '<', '/', 'g', '>', 'ìè§']\",80510,NÍú•w,PROPN,Sich windender (Personifikation der Schlange)\n",
    "200,67HZI45S3REA3LWVZOKJ6QJOIE,14,9,67HZI45S3REA3LWVZOKJ6QJOIE.14.9,nbiÃØ.n,nbiÃØ.n,ìàñìéüìÉÄ<g>D107</g>ìàñ,\"['ìàñ', 'ìéü', 'ìÉÄ', '<', 'g', '>', 'D', '1', '0', '7', '<', '/', 'g', '>', 'ìàñ']\",82520,nbiÃØ,VERB,schmelzen; gie√üen\n",
    "204,67HZI45S3REA3LWVZOKJ6QJOIE,14,13,67HZI45S3REA3LWVZOKJ6QJOIE.14.13,n·∏èr.n,n·∏èr.n,ìàñìá¶ìÇã<g>U19A</g>ìÜ±ìàñ,\"['ìàñ', 'ìá¶', 'ìÇã', '<', 'g', '>', 'U', '1', '9', 'A', '<', '/', 'g', '>', 'ìÜ±', 'ìàñ']\",91630,n·∏èr,VERB,(Holz) bearbeiten; zimmern\n",
    "206,67HZI45S3REA3LWVZOKJ6QJOIE,14,15,67HZI45S3REA3LWVZOKJ6QJOIE.14.15,b(w)n.wDU,bwn.wDU,ìÉÄìàñìèåìÖ±<g>T86</g><g>T86</g>,\"['ìÉÄ', 'ìàñ', 'ìèå', 'ìÖ±', '<', 'g', '>', 'T', '8', '6', '<', '/', 'g', '>', '<', 'g', '>', 'T', '8', '6', '<', '/', 'g', '>']\",55330,bwn,NOUN,Speerspitzen (des Fischspeeres)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T09:02:49.014542Z",
     "start_time": "2024-05-01T09:02:49.014528Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "# Convert the string into a StringIO object\n",
    "# This is only necessary because we presented the csv as a string not as a file that is loaded into the notebook\n",
    "csv_data = StringIO(eg2_csv)\n",
    "\n",
    "# Read the data into a pandas DataFrame\n",
    "df = pd.read_csv(csv_data)\n",
    "\n",
    "# Display the DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T09:02:49.015676Z",
     "start_time": "2024-05-01T09:02:49.015667Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def split_tags(text):\n",
    "    parts = []  # List to collect output of the function\n",
    "    while '<g>' in text and '</g>' in text:\n",
    "        pre, rest = text.split('<g>', 1)  # splits at the first <g> found\n",
    "        tag_content, post = rest.split('</g>', 1)  # splits the rest at the first </g> found\n",
    "\n",
    "        # adds elements before the first <g></g> tag to the List\n",
    "        parts.extend(pre)\n",
    "\n",
    "        #  adds element inside the first <g></g> tag to the List\n",
    "        parts.append(tag_content)\n",
    "\n",
    "        # text variable is set to remaining text\n",
    "        text = post\n",
    "\n",
    "    # After last tag found, the remainder of the text is split and added to the List\n",
    "    parts.extend(text)\n",
    "    return parts\n",
    "\n",
    "def process_text(text):\n",
    "    if pd.isna(text): # deals with NaN\n",
    "        return []\n",
    "    else:\n",
    "        return split_tags(text)\n",
    "\n",
    "# apply functions to every row of the column 'unicode_word'\n",
    "df['unicode_splitted'] = df['unicode_word'].apply(process_text)\n",
    "# delete obsolete column\n",
    "df.drop('unicode', axis=1, inplace=True)\n",
    "\n",
    "df\n",
    "#df.to_csv(\"EG-TLA-example.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
