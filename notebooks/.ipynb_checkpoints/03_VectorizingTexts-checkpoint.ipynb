{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "CYpMtqhKV_Hr",
   "metadata": {
    "id": "CYpMtqhKV_Hr"
   },
   "source": [
    "\n",
    "# Exploring Texts using the Vector Space Model\n",
    "**Text and code copied from:**</br> [Karsdorp, F., Kestemont, M., & Riddell, A. (2021). *Humanities Data Analysis: Case Studies with Python*, Princeton University Press.](https://www.humanitiesdataanalysis.org/vector-space-model/notebook.html)</br>\n",
    "Adapted by Eliese-Sophia Lincke & Shai Gordin for the purposes of the course [Ancient Language Processing](https://digitalpasts.github.io/ALP-course/) (summer term 2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a87ae1b",
   "metadata": {
    "id": "8a87ae1b",
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np # need for log() function\n",
    "import pandas as pd\n",
    "import collections\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb47ba6",
   "metadata": {
    "id": "0cb47ba6"
   },
   "source": [
    "## From Texts to Vectors: TF-IDF\n",
    "\n",
    "When using the <span class=\"index\">vector space model</span>, a <span\n",
    "class=\"index\">corpus</span>---a collection of documents, each represented as a bag of\n",
    "words---is typically represented as a matrix, in which each row represents a document from\n",
    "the collection, each column represents a word from the collection's <span\n",
    "class=\"index\">vocabulary</span>, and each cell represents the frequency with which a\n",
    "particular word occurs in a document.\n",
    "\n",
    "A matrix arranged in this way is often called a *document-term matrix*---or *term-document matrix* where:\n",
    "* rows are associated with documents\n",
    "* word counts are in the columns.\n",
    "\n",
    "```{table} Example of a vector space representation with four documents (rows) and a vocabulary of four words (columns). For each document the table lists how often each vocabulary item occurs.\n",
    "\n",
    "|       | _roi_ | _ange_ | _sang_ | _perdu_ |\n",
    "|-------|-------|--------|--------|---------|\n",
    "| $d_1$ |     1 |      2 |     16 |      21 |\n",
    "| $d_2$ |     2 |      2 |     18 |      19 |\n",
    "| $d_3$ |    35 |     41 |      0 |       2 |\n",
    "| $d_4$ |    39 |     55 |      1 |       0 |\n",
    "```\n",
    "\n",
    "In this table, each document $d_i$ is represented as a vector, which, essentially, is a list of numbers---word frequencies in our present case. A <span class=\"index\">vector space</span> is nothing more than a collection of numerical vectors, which may, for instance, be added together and multiplied by a number. Documents represented in this manner may be compared in terms of their *coordinates* (or *components*). For example, by comparing the four documents on the basis of the second coordinate, we observe that the first two documents ($d_1$ and $d_2$) have similar counts, which might be an indication that these two documents are somehow more similar. To obtain a more accurate and complete picture of document similarity, we would like to be able to compare documents more holistically, using *all* their components. In our example, each document represents a point in a four-dimensional vector space. We might hypothesize that similar documents use similar words, and hence reside close to each other in this space. To illustrate this, we demonstrate how to visualize the documents in space using the first and third components.\n",
    "\n",
    "![](https://www.humanitiesdataanalysis.org/_images/notebook_2_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TGm5abWqqA1k",
   "metadata": {
    "id": "TGm5abWqqA1k"
   },
   "source": [
    "\n",
    "* TF-IDF: Concept\n",
    "![](https://www.romainberg.com/wp-content/uploads/TF_IDF-final.png)\n",
    "\n",
    "* TF-IDF: Simple calculation\n",
    "![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wQ-FCEYoioz6tOlYFssXeg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-e54VNL_qsmB",
   "metadata": {
    "id": "-e54VNL_qsmB"
   },
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sCB5jEe2jaAp",
   "metadata": {
    "id": "sCB5jEe2jaAp"
   },
   "outputs": [],
   "source": [
    "# raw data\n",
    "\n",
    "akk05 = ['ana', 'eššūtu', 'kašāru', 'mātu', 'Aššur', 'ana', 'UNK', 'ālu', 'epēšu', 'ēkallu', 'mūšabu', 'šarrūtu', 'ina', 'libbu', 'nadû', 'UNK', 'šumu', 'nabû', 'kakku', 'Aššur', 'bēlu', 'ina', 'libbu', 'ramû', 'nišu', 'mātu', 'kišittu', 'qātu', 'ina', 'libbu', 'wašābu', 'biltu', 'maddattu', 'kânu', 'itti', 'nišu', 'mātu', 'Aššur', 'manû', 'ṣalmu', 'šarrūtu', 'u', 'ṣalmu', 'ilu', 'rabû', 'bēlu', 'epēšu', 'lītu', 'u', 'danānu', 'ša', 'ina', 'zikru', 'Aššur', 'bēlu', 'eli', 'mātu', 'šakānu', 'ina', 'muhhu', 'šaṭāru', 'ina', 'UNK', 'izuzzu', 'UNK', 'UNK', 'biltu', 'hurāṣu', 'ina', 'dannu', 'UNK', 'līm', 'biltu', 'kaspu', 'UNK', 'maddattu', 'mahāru', 'ina', 'UNK', 'palû', 'Aššur', 'bēlu', 'takālu', 'ana', 'Namri', 'UNK', 'Bit-Zatti', 'Bit-Abdadani', 'Bit-Sangibuti', 'UNK', 'alāku', 'UNK', 'akāmu', 'gerru', 'amāru', 'Nikkur', 'ālu', 'dannūtu', 'wašāru', 'UNK', 'zanānu', 'Nikkurayu', 'kakku', 'UNK', 'sisû', 'parû', 'alpu', 'UNK', 'Sassiašu', 'Tutašdi', 'UNK']\n",
    "akk08 = ['UNK', 'nišu', 'ana', 'mātu', 'Aššur', 'warû', 'UNK', 'UNK', 'ina', 'UNK', 'palû', 'Aššur', 'bēlu', 'takālu', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'Sulumal', 'Meliddayu', 'Tarhu-lara', 'Gurgumayu', 'UNK', 'UNK', 'UNK', 'mātitān', 'ana', 'emūqu', 'ahāmiš', 'takālu', 'UNK', 'UNK', 'ina', 'lītu', 'u', 'danānu', 'ša', 'Aššur', 'bēlu', 'itti', 'mahāṣu', 'dīktu', 'dâku', 'UNK', 'UNK', 'qurādu', 'dâku', 'hurru', 'natbāku', 'šadû', 'malû', 'narkabtu', 'UNK', 'UNK', 'ana', 'lā', 'mānu', 'leqû', 'ina', 'qablu', 'tidūku', 'ša', 'Ištar-duri', 'UNK', 'UNK', 'UNK', 'UNK', 'ina', 'qātu', 'ṣabātu', 'UNK', 'līm', 'UNK', 'līm', 'UNK', 'meʾatu', 'UNK', 'UNK', 'UNK', 'UNK', 'ištu', 'UNK', 'UNK', 'Ištar-duri', 'ana', 'ezēbu', 'napištu', 'mūšiš', 'halāqu', 'lāma', 'šamšu', 'urruhiš', 'naprušu', 'UNK', 'UNK', 'itti', 'šiltāhu', 'pāriʾu', 'napištu', 'adi', 'titūru', 'Purattu', 'miṣru', 'mātu', 'ṭarādu', 'eršu', 'UNK', 'UNK', 'ša', 'šadādu', 'šarrūtu', 'kunukku', 'kišādu', 'adi', 'abnu', 'kišādu', 'narkabtu', 'šarrūtu', 'UNK', 'UNK', 'mimma', 'šumu', 'mādu', 'ša', 'nību', 'lā', 'išû', 'ekēmu', 'sisû', 'UNK', 'UNK', 'ummiānu', 'ana', 'lā', 'mānu', 'leqû', 'bītu', 'ṣēru', 'kuštāru', 'šarrūtu', 'UNK', 'UNK', 'unūtu', 'tāhāzu', 'mādu', 'ina', 'qerbu', 'ušmannu', 'ina', 'išātu', 'šarāpu', 'UNK', 'UNK', 'UNK', 'eršu', 'ana', 'Ištar', 'šarratu', 'Ninua', 'qiāšu', 'UNK']\n",
    "akk11 = ['mašku', 'pīru', 'šinnu', 'pīru', 'argamannu', 'takiltu', 'lubuštu', 'birmu', 'kitû', 'lubuštu', 'mātu', 'mādu', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'tillu', 'pilaqqu', 'UNK', 'UNK', 'UNK', 'UNK', 'ina', 'qerbu', 'Arpadda', 'mahāru', 'Tutammu', 'šarru', 'Unqi', 'ina', 'adû', 'ilu', 'rabû', 'haṭû', 'šiāṭu', 'napištu', 'gerru', 'UNK', 'lā', 'malāku', 'itti', 'ina', 'uzzu', 'libbu', 'UNK', 'ša', 'Tutammu', 'adi', 'rabû', 'UNK', 'Kunalua', 'ālu', 'šarrūtu', 'kašādu', 'nišu', 'adi', 'maršītu', 'UNK', 'kūdanu', 'ina', 'qerbu', 'ummānu', 'kīma', 'ṣēnu', 'manû', 'UNK', 'ina', 'qabaltu', 'ēkallu', 'ša', 'Tutammu', 'kussû', 'nadû', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'meʾatu', 'biltu', 'kaspu', 'ina', 'dannu', 'UNK', 'meʾatu', 'biltu', 'UNK', 'UNK', 'unūtu', 'tāhāzu', 'lubuštu', 'birmu', 'kitû', 'rīqu', 'kalāma', 'būšu', 'ēkallu', 'UNK', 'Kunalua', 'ana', 'eššūtu', 'ṣabātu', 'Unqi', 'ana', 'pāṭu', 'gimru', 'kanāšu', 'UNK', 'šūt', 'rēšu', 'bēlu', 'pīhātu', 'eli', 'šakānu']\n",
    "akk13 = ['UNK', 'ana', 'Hatti', 'adi', 'mahru', 'wabālu', 'šūt', 'rēšu', 'šaknu', 'mātu', 'Naʾiri', 'Supurgillu', 'UNK', 'adi', 'ālu', 'ša', 'liwītu', 'kašādu', 'šallatu', 'šalālu', 'Šiqila', 'rabû', 'birtu', 'UNK', 'šalālu', 'ana', 'Hatti', 'adi', 'mahru', 'wabālu', 'UNK', 'meʾatu', 'šallatu', 'Amlate', 'ša', 'Damunu', 'UNK', 'līm', 'UNK', 'meʾatu', 'šallatu', 'Deri', 'ina', 'Kunalua', 'UNK', 'Huzarra', 'Taʾe', 'Tarmanazi', 'Kulmadara', 'Hatatirra', 'Irgillu', 'ālu', 'ša', 'Unqi', 'wašābu', 'UNK', 'šallatu', 'Qutu', 'Bit-Sangibuti', 'UNK', 'līm', 'UNK', 'meʾatu', 'Illilayu', 'UNK', 'līm', 'UNK', 'meʾatu', 'UNK', 'Nakkabayu', 'Budayu', 'ina', 'UNK', 'Ṣimirra', 'Arqa', 'Usnu', 'Siʾannu', 'ša', 'šiddu', 'tiāmtu', 'wašābu', 'UNK', 'meʾatu', 'UNK', 'Budayu', 'Dunu', 'UNK', 'UNK', 'UNK', 'UNK', 'meʾatu', 'UNK', 'Belayu', 'UNK', 'meʾatu', 'UNK', 'Banitayu', 'UNK', 'meʾatu', 'UNK', 'Palil-andil-mati', 'UNK', 'meʾatu', 'UNK', 'Sangillu', 'UNK', 'Illilayu', 'UNK', 'meʾatu', 'UNK', 'šallatu', 'Qutu', 'Bit-Sangibuti', 'ina', 'pīhātu', 'Tuʾimmu', 'wašābu', 'UNK', 'meʾatu', 'UNK', 'šallatu', 'Qutu', 'Bit-Sangibuti', 'ina', 'Til-karme', 'wašābu', 'itti', 'nišu', 'mātu', 'Aššur', 'manû', 'ilku', 'tupšikku', 'kī', 'ša', 'Aššuru', 'emēdu', 'maddattu', 'ša', 'Kuštašpi', 'Kummuhayu', 'Rahianu', 'Ša-imerišayu', 'Menaheme', 'Samerinayu', 'Hi-rumu', 'Ṣurrayu', 'Sibitti-Biʾil', 'Gublayu', 'Uriaikki', 'Quayu', 'Pisiris', 'Gargamišayu', 'Eni-il', 'Hamatayu', 'Panammu', 'Samʾallayu', 'Tarhu-lara', 'Gurgumayu', 'Sulumal', 'Meliddayu', 'Dadilu']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lFJGey8pOCz6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lFJGey8pOCz6",
    "outputId": "6d562541-9b96-4a71-dcef-ef34e2558c27"
   },
   "outputs": [],
   "source": [
    "# returns a list of lists, each lists is one document in the corpus\n",
    "\n",
    "tokenized_corpus = []\n",
    "for doc in [akk05, akk08, akk11, akk13] :\n",
    "  akk = []\n",
    "  for token in doc :\n",
    "    akk.append(token)#.lower())\n",
    "  tokenized_corpus.append(akk)\n",
    "\n",
    "for doc in tokenized_corpus :\n",
    "  print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5846a276",
   "metadata": {
    "id": "5846a276"
   },
   "source": [
    "`Counter` implements a number of methods specialized for convenient and rapid tallies. For instance, the method <span class=\"index\">`Counter.most_common`</span> returns the *n* most frequent items:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iPmd7KYikoOF",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iPmd7KYikoOF",
    "outputId": "fdb07387-e954-4d76-948f-8617d4a67059"
   },
   "outputs": [],
   "source": [
    "# Count token frequencies\n",
    "\n",
    "vocabulary_akk = Counter(akk05)\n",
    "print(vocabulary_akk)\n",
    "print(vocabulary_akk.most_common(n=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FafxQzOwhvJf",
   "metadata": {
    "id": "FafxQzOwhvJf"
   },
   "source": [
    "### Calculations of the components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pECF9IHxMS1C",
   "metadata": {
    "id": "pECF9IHxMS1C"
   },
   "source": [
    "#### Extract a vocabulary (the inventory of types/lemmata) from a corpus\n",
    "\n",
    "  * Arguments:\n",
    "      * `tokenized_corpus` (list): a tokenized corpus represented, list of lists of strings.\n",
    "      * `min_count` (int, optional): the minimum occurrence count of a vocabulary item in the corpus.\n",
    "      * `max_count` (int, optional): the maximum occurrence count of a vocabulary item in the corpus. Note that the default maximum count is set to infinity (max_count=float('inf')). This ensures that none of the high-frequency words are filtered without further specification.\n",
    "\n",
    "  * Returns:\n",
    "      * `list`: an alphabetically ordered list of unique words in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcd38e6",
   "metadata": {
    "id": "9bcd38e6"
   },
   "outputs": [],
   "source": [
    "def extract_vocabulary(tokenized_corpus, min_count=1, max_count=float('inf')):\n",
    "\n",
    "    vocabulary = collections.Counter()\n",
    "    for document in tokenized_corpus:\n",
    "        vocabulary.update(document)\n",
    "    vocabulary = {word for word, count in vocabulary.items()\n",
    "                  if count >= min_count and count <= max_count}\n",
    "    return sorted(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "m1TR-vnHM_0G",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m1TR-vnHM_0G",
    "outputId": "93cdf8a1-bdd8-4d2a-bd4d-ce1e3f5a21aa"
   },
   "outputs": [],
   "source": [
    "# Call the function\n",
    "\n",
    "vocabulary = extract_vocabulary(tokenized_corpus, min_count = 1)\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f4476d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "66f4476d",
    "outputId": "7d08e36c-a70d-4677-e55f-463121fcb24a"
   },
   "outputs": [],
   "source": [
    "# Check token counts for each type in the vocabulary\n",
    "bags_of_words = []\n",
    "for document in tokenized_corpus:\n",
    "    tokens = [word for word in document if word in vocabulary]\n",
    "    bags_of_words.append(collections.Counter(tokens))\n",
    "    #bags_of_words.extend(collections.Counter(tokens))\n",
    "\n",
    "for count in bags_of_words :\n",
    "    print(count)\n",
    "#print(bags_of_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b19ff4",
   "metadata": {
    "id": "c3b19ff4"
   },
   "source": [
    "#### Calculate the *term frequency* (TF)\n",
    "\n",
    "Transform a tokenized corpus into a document-term matrix.\n",
    "\n",
    "  * Arguments:\n",
    "      * `tokenized_corpus` (list): a tokenized corpus as a list of lists of strings.\n",
    "      * `vocabulary` (list): A list of unique words (types).\n",
    "\n",
    "  * Returns:\n",
    "      * `list`: A list of lists representing the frequency of each term in `vocabulary` for each document in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877d3254",
   "metadata": {
    "id": "877d3254"
   },
   "outputs": [],
   "source": [
    "## Calculate term frequency (TF)\n",
    "# raw count\n",
    "\n",
    "def corpus2dtm_raw(tokenized_corpus, vocabulary):\n",
    "\n",
    "    document_term_matrix = []\n",
    "    for document in tokenized_corpus:\n",
    "        document_counts = collections.Counter(document)\n",
    "        row = [document_counts[word] for word in vocabulary]\n",
    "        document_term_matrix.append(row)\n",
    "    return document_term_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZxvGXwvFQbyw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 259
    },
    "id": "ZxvGXwvFQbyw",
    "outputId": "16b608ad-7eb8-445e-d79c-0f6ecd192824"
   },
   "outputs": [],
   "source": [
    "# Call the function\n",
    "document_term_matrix = corpus2dtm_raw(tokenized_corpus, vocabulary)\n",
    "print(document_term_matrix)\n",
    "\n",
    "# Convert result into a dataframe\n",
    "tf_df_abs = pd.DataFrame(document_term_matrix, columns=vocabulary)\n",
    "\n",
    "tf_df_abs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5815e8-125c-4d1e-84fb-5da4cf811133",
   "metadata": {
    "id": "TBfxW0uM_PRU"
   },
   "source": [
    "There are three (and possibly more) ways to calculate the TF:\n",
    "* **raw count** (token count) -- like in the function above </br>\n",
    "This is the simplest form, where TF is just the raw count of the term in the document:</br>\n",
    "`TF(t,d) = count of term t in document d`\n",
    "* **relative frequency** (token count / number of tokens in the document)</br>\n",
    "TF is normalized by dividing the raw count by the total number of terms in the document: </br>\n",
    "`TF(t,d) = count of term t in document d / total number of tokens in document d`\n",
    "* **logarithmically scaled**: typically involves a normalization step to account for the length of the document. This reduces the impact of (very) frequent terms.</br>\n",
    "`TF(t,d) = log(count of term t in document d + 1)` <br/>\n",
    "(When the term frequency is 0, `+ 1` avoids `log(0)` which would result in an error.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5Pn8R2zX_xim",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5Pn8R2zX_xim",
    "outputId": "fa545b8d-e62e-40e1-dad1-ecb1f97050e7"
   },
   "outputs": [],
   "source": [
    "# Demonstration of different calculations of the term frequency\n",
    "\n",
    "def raw_count_tf(term, document):\n",
    "    return document.count(term)\n",
    "\n",
    "def relative_frequency_tf(term, document):\n",
    "    term_count = document.count(term)\n",
    "    total_terms = len(document)\n",
    "    return term_count / total_terms if total_terms > 0 else 0 # avoid division by 0\n",
    "\n",
    "def log_scaled_tf(term, document):\n",
    "    total_terms = len(document)\n",
    "    term_count = document.count(term)\n",
    "    return np.log(1 + term_count) # avoid log(0) by adding 1\n",
    "\n",
    "# Example document\n",
    "document = [\"this\", \"is\", \"a\", \"sample\", \"document\", \"document\", \"is\", \"a\", \"sample\", \"sample\", \"sample\", \"sample\", \"sample\", \"sample\"]\n",
    "\n",
    "term = \"sample\" # change to \"sample\" to see the scaling down effect for frequent terms\n",
    "\n",
    "print(\"Raw Count TF:\", raw_count_tf(term, document))\n",
    "print(\"Relative Frequency TF:\", relative_frequency_tf(term, document))\n",
    "print(\"Log Scaled TF:\", log_scaled_tf(term, document)) # natural base of logarithm (e = \"Euler's number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25Zswsq0i8cO",
   "metadata": {
    "id": "25Zswsq0i8cO"
   },
   "outputs": [],
   "source": [
    "## Calculate term frequency (TF) for our corpus\n",
    "# relative frequency\n",
    "\n",
    "def corpus_tf(tokenized_corpus, vocabulary):\n",
    "    document_term_matrix = []\n",
    "    for document in tokenized_corpus:\n",
    "        term_per_document_counts = collections.Counter(document)\n",
    "        total_terms = sum(term_per_document_counts.values())\n",
    "        #total_terms = len(document_counts)\n",
    "        #row = [term_per_document_counts[word] for word in vocabulary]\n",
    "        row = [np.log(1 + term_per_document_counts[word]) for word in vocabulary]\n",
    "        document_term_matrix.append(row)\n",
    "    return document_term_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Sfawn9zWVBce",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 242
    },
    "id": "Sfawn9zWVBce",
    "outputId": "92defa7f-25a4-437b-8941-5eb34024a2b8"
   },
   "outputs": [],
   "source": [
    "# Call the function\n",
    "\n",
    "# Calculate term frequency (TF) document-term matrix\n",
    "term_frequency_matrix = corpus_tf(tokenized_corpus, vocabulary)\n",
    "\n",
    "# Convert the matrix to a DataFrame for easier visualization\n",
    "tf_df_log = pd.DataFrame(term_frequency_matrix, columns=vocabulary)\n",
    "\n",
    "tf_df_log#[\"libbu\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52b8199",
   "metadata": {
    "id": "d52b8199"
   },
   "source": [
    "#### Calculate the *Inverse Document Frequency* (IDF)\n",
    "* **N** = number of documents (in the corpus) <br/>\n",
    "* **df_term_counts** = number of documents which contain term t \n",
    "* **absolute** <br/> `IDF(t) = number of documents / number of documents containing term t`\n",
    "* **logarithmic** <br/> `IDF(t) = log(number of documents / number of documents containing term t + 1)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cRFA1n-i6Jr_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 150
    },
    "id": "cRFA1n-i6Jr_",
    "outputId": "84ecc9d8-f860-45cd-cb53-2494402ad6aa"
   },
   "outputs": [],
   "source": [
    "# Step 1: Calculate the total number of documents (N): How many documents are there in the corpus?\n",
    "N = len(tf_df_abs)\n",
    "print(N)\n",
    "\n",
    "# Step 2: Calculate the document frequency (DF) for each term: In how many documents does the term appear?\n",
    "df_nonzero = tf_df_abs > 0  # Convert counts to binary (True/False)\n",
    "#print(df_nonzero)\n",
    "df_term_counts = df_nonzero.sum(axis=0)  # Sum across rows, i.e. across documents; rows/documents set to 'False' are not counted\n",
    "print(df_term_counts)\n",
    "\n",
    "# Step 3: Calculate the IDF for each term\n",
    "# add 1 for normalization\n",
    "idf = np.log(N / (df_term_counts + 1 )) + 1 # natural base (\"Euler's number\")\n",
    "\n",
    "# Display the IDF values\n",
    "idf_df = pd.DataFrame(idf, columns=['IDF']).T\n",
    "idf_df.loc['df_term_count'] = df_term_counts\n",
    "idf_df#.iloc[:, 90:105]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MVFcRzh7XKM4",
   "metadata": {
    "id": "MVFcRzh7XKM4"
   },
   "source": [
    "### Putting it all together: the TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-4u7UdU9heu3",
   "metadata": {
    "id": "-4u7UdU9heu3"
   },
   "source": [
    "#### Simple TF-IDF\n",
    "Multiply term frequency and inverse document frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fXbwK7STA41a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 242
    },
    "id": "fXbwK7STA41a",
    "outputId": "cbedf116-f2e6-4684-9745-fbd9bd109651"
   },
   "outputs": [],
   "source": [
    "# Perform element-wise multiplication\n",
    "tf_idf_df = tf_df_log * idf_df.loc['IDF']\n",
    "\n",
    "#tf_idf_df.iloc[:, 90:105]\n",
    "tf_idf_df#[\"libbu\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5C9vcdXthiw-",
   "metadata": {
    "id": "5C9vcdXthiw-"
   },
   "source": [
    "#### Weighted TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ALHwhXN4tgA8",
   "metadata": {
    "id": "ALHwhXN4tgA8"
   },
   "outputs": [],
   "source": [
    "def calculate_tf_weighted(term, document):\n",
    "    term_count = document.count(term)\n",
    "    if term_count > 0:\n",
    "        tf = 1 + np.log(term_count)\n",
    "    else:\n",
    "        tf = 0\n",
    "    return tf\n",
    "\n",
    "def calculate_idf_weighted(term, corpus):\n",
    "    num_documents_with_term = sum(1 for doc in corpus if term in doc)\n",
    "    idf = np.log((len(corpus) + 1) / (num_documents_with_term + 1)) + 1\n",
    "    return idf\n",
    "\n",
    "def calculate_tf_idf_weighted(term, document, corpus):\n",
    "    tf = calculate_tf_weighted(term, document)\n",
    "    idf = calculate_idf_weighted(term, corpus)\n",
    "    tf_idf = tf * idf\n",
    "    return tf_idf\n",
    "\n",
    "# Define a function to calculate the weighted TF-IDF for every document in the corpus\n",
    "def calculate_tf_idf_matrix(tokenized_corpus, vocabulary):\n",
    "    tf_idf_matrix = []\n",
    "    for document in tokenized_corpus:\n",
    "        row = [calculate_tf_idf_weighted(term, document, tokenized_corpus) for term in vocabulary]\n",
    "        tf_idf_matrix.append(row)\n",
    "    return tf_idf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Y_jGpUnomW4s",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 242
    },
    "id": "Y_jGpUnomW4s",
    "outputId": "fbd8cee2-0361-4243-aee9-8cbaf6637f79"
   },
   "outputs": [],
   "source": [
    "#Calculate the TF-IDF weighted matrix for the entire corpus\n",
    "tf_idf_matrix = calculate_tf_idf_matrix(tokenized_corpus, vocabulary)\n",
    "\n",
    "# Step 3: Convert the matrix to a DataFrame\n",
    "tf_idf_df = pd.DataFrame(tf_idf_matrix, columns=vocabulary)\n",
    "\n",
    "# Display the resulting TF-IDF DataFrame\n",
    "tf_idf_df#[\"Aššur\"]#[\"libbu\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RmeMdITenBY_",
   "metadata": {
    "id": "RmeMdITenBY_"
   },
   "source": [
    "### Predefined TF-IDF functions from scikit-learn\n",
    "\n",
    "In the main code of the course, we will use a predefined function from the machine learning library `scikit-learn`.\n",
    "\n",
    "[Learn more about the weighting and normalization in scikit-learns TF-IDF calculations](https://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting), </br>\n",
    "cf. in particular the \"Numeric example of a tf-idf matrix\"\n",
    "\n",
    "`TfidfVectorizer` requires a list of strings as input. Each string is an entire text (document)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0rZa61uGFkF6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0rZa61uGFkF6",
    "outputId": "15612658-ec79-417b-e0e7-59c65946be69"
   },
   "outputs": [],
   "source": [
    "## Alternative preprocessing for the TfidfVectorizer from SciKitLearn\n",
    "# returns a list of strings, each string is one document\n",
    "\n",
    "tokenized_corpus_asStr = []\n",
    "for doc in [akk05, akk08, akk11, akk13] :\n",
    "  akk = \"\"\n",
    "  for token in doc :\n",
    "    akk = akk + token + \" \"\n",
    "  tokenized_corpus_asStr.append(akk.strip())\n",
    "\n",
    "for doc in tokenized_corpus_asStr :\n",
    "  print(doc + \" ------\")\n",
    "#print(tokenized_corpus_asStr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nvCxnJxs3k0v",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "id": "nvCxnJxs3k0v",
    "outputId": "ce84f7c4-3e9b-4047-81eb-dbaaebb65b67"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "# Step 1: Instantiate TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(min_df = 0, max_df = 100, norm = \"l1\") # with and without normalization (norm = None)\n",
    "\n",
    "# Step 2: Fit and transform the corpus\n",
    "tfidf_matrix = vectorizer.fit_transform(tokenized_corpus_asStr)\n",
    "\n",
    "# Display the TF-IDF matrix\n",
    "#tfidf_matrix.toarray()\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "tfidf_df#[\"aššur\"]#[\"libbu\"]\n",
    "\n",
    "# Display the feature names\n",
    "#vectorizer.get_feature_names_out()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.10.3"
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "source_map": [
   16,
   20,
   81,
   100,
   127,
   138,
   142,
   145,
   153,
   162,
   168,
   200,
   206,
   219,
   223,
   250,
   254,
   258,
   262,
   268,
   272,
   274,
   278,
   283,
   287,
   292,
   296,
   327,
   331,
   334,
   338,
   345,
   349,
   379,
   385,
   389,
   400,
   405,
   441,
   467,
   472,
   480,
   493,
   501,
   507,
   513,
   519,
   521,
   532,
   542,
   565,
   569,
   573,
   575,
   579,
   584,
   589,
   601,
   610,
   614,
   619,
   644,
   667,
   690,
   694,
   703,
   710,
   779,
   812,
   816,
   822,
   845,
   849,
   858,
   887,
   907,
   911,
   953,
   963,
   972,
   985,
   998,
   1015,
   1040,
   1042,
   1048,
   1051,
   1055,
   1058,
   1064,
   1068,
   1072,
   1080,
   1084,
   1088,
   1101,
   1104,
   1108,
   1111,
   1138,
   1147,
   1208,
   1210,
   1215,
   1217,
   1221,
   1223,
   1227,
   1230,
   1234,
   1237,
   1241,
   1244,
   1248,
   1251,
   1255,
   1258,
   1262,
   1265,
   1271,
   1273,
   1277,
   1279,
   1283,
   1287,
   1289,
   1297,
   1301,
   1303,
   1307,
   1310,
   1317,
   1320,
   1324,
   1326,
   1336,
   1340,
   1346,
   1348,
   1352,
   1354,
   1358,
   1360,
   1364,
   1366,
   1370,
   1372,
   1381,
   1383,
   1387,
   1391,
   1395,
   1397,
   1401,
   1403,
   1407,
   1410,
   1418,
   1421,
   1425,
   1428,
   1432,
   1435,
   1439,
   1442,
   1446,
   1448,
   1452,
   1454,
   1462,
   1465,
   1469,
   1471,
   1475,
   1478,
   1493,
   1495,
   1499,
   1501,
   1505,
   1507,
   1516,
   1520,
   1524,
   1527
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
